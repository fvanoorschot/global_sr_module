{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62e3f38f-c900-4727-a9e1-2735ae539100",
   "metadata": {},
   "source": [
    "# Main run script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9107eedc-c820-4eba-87b3-c52f84faf2fa",
   "metadata": {},
   "source": [
    "This script contains the main procedure to calculate global root zone storage capacities. \n",
    "\n",
    "This scripts only works in the conda environment **sr_env**. In this environment all required packages are available. If you have **not** installed and activated this environment before opening this script, you should check the installation section in the *README* file. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d86bf57-168c-4591-9050-c261b4adaafd",
   "metadata": {},
   "source": [
    "### 1. Getting started\n",
    "First, import all the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12476d9c-f4d9-41d7-889e-4e84ec7ee35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import calendar\n",
    "import geopandas as gpd\n",
    "import cartopy\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from scipy.optimize import least_squares\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import gaussian_kde\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from statsmodels.tools import add_constant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d94080-a55a-49dc-bfd7-09ef86d1ffa2",
   "metadata": {},
   "source": [
    "Here we import all the python functions defined in the scripts *f_catch_characteristics.py* and *f_preprocess_discharge.py*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b053a7a-23b8-434d-ae1d-19888e5f5494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import python functions\n",
    "from f_catch_characteristics import *\n",
    "from f_preprocess_discharge import *\n",
    "from f_sr_calculation import *\n",
    "from f_regression import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a9ed88-646a-4209-b0a1-5e01426119ee",
   "metadata": {},
   "source": [
    "### 2. Define working and data directories\n",
    "Here we define the working directory, where all the scripts and output are saved.\n",
    "\n",
    "We also define the data directory where you have the following subdirectories:\n",
    "\n",
    "/data/forcing/*netcdf forcing files*\\\n",
    "/data/shapes/*catchment shapefiles*\\\n",
    "/data/gsim_discharge/*gsim discharge timeseries*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e8fa1bc-52f6-4907-ac52-d181690a37a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/work/users/vanoorschot/fransje/scripts/global_sr_module/scripts'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check current working directory (helpful when filling in work_dir below)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48e094df-ec04-4c83-a259-9a4395300ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define your script working directory\n",
    "# work_dir=Path(\"/home/fransjevanoors/global_sr_module\")\n",
    "work_dir=Path(\"/work/users/vanoorschot/fransje/scripts/global_sr_module\")\n",
    "# define your data directory\n",
    "data_dir=Path(\"/work/users/vanoorschot/fransje/scripts/global_sr_module/data_example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e725c95a-41c4-41ee-8e26-a2d21cecd45b",
   "metadata": {},
   "source": [
    "Here we create the output directory inside your working directory. In the remainder of this module, the same command will be used regularly to create directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28501b38-7d4c-4c8d-9e0a-237aa5605dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make output directory\n",
    "if not os.path.exists(f'{work_dir}/output_example'):\n",
    "    os.makedirs(f'{work_dir}/output_example')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9e0fa0d-eaa1-421c-ab69-c1f4e8c015c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = Path(f\"{work_dir}/output_example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140fd899-95d5-4490-b13c-aa448ba8e410",
   "metadata": {},
   "source": [
    "### 3. Make lists of catchment IDs\n",
    "The module calculates catchment root zone storage capacities for a large sample of catchments. Here we save the catchment names in a .txt file, for later use in the scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47d31e59-49f8-4e94-af71-60abddffb7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the filenames of catchment shapefiles\n",
    "shape_dir = Path(f'{data_dir}/shapes/')\n",
    "shapefiles = glob.glob(f\"{shape_dir}/*shp\")\n",
    "\n",
    "# make an empty list\n",
    "catch_id_list = []\n",
    "\n",
    "# loop over the catchment shapefiles, extract the catchment id and store in the empty list\n",
    "for i in shapefiles:\n",
    "    catch_id_list.append(Path(i).name.split('.')[0])\n",
    "    \n",
    "# save the catchment id list in your output directory\n",
    "np.savetxt(f'{out_dir}/catch_id_list.txt',catch_id_list,fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3c8f9cd-45af-4bc8-bdf8-88374d7c30db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['br_0000495', 'fr_0000326', 'us_0002247', 'br_0000208', 'ca_0002435', 'zw_0000005']\n"
     ]
    }
   ],
   "source": [
    "# print catch_id_list as check\n",
    "print(catch_id_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baf7036-1515-444d-83f0-b7d2b6bead68",
   "metadata": {},
   "source": [
    "### 4. GSIM discharge data\n",
    "### 4.1 Preprocess data \n",
    "\n",
    "The GSIM yearly discharge timeseries are stored in *.year* files. A detailed explanation of the column names is provided in Table 3 and 4 in https://essd.copernicus.org/articles/10/787/2018/. Here we preprocess these data into readable *.csv* files for each catchment. The preprocessing function *preprocess_gsim_discharge* is defined in the file *f_preprocess_discharge.py*. With this function we generate for each catchment a file with the yearly discharge timeseries and a file with the specifications of the catchment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a50094bb-f752-4e43-8dd9-b716cb5f0dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make output directories\n",
    "if not os.path.exists(f'{out_dir}/discharge/timeseries'):\n",
    "    os.makedirs(f'{out_dir}/discharge/timeseries')\n",
    "    \n",
    "if not os.path.exists(f'{out_dir}/discharge/characteristics'):\n",
    "    os.makedirs(f'{out_dir}/discharge/characteristics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6c3a68a-ac3b-4897-88e6-d39721d5335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define folder with discharge timeseries data\n",
    "fol_in = f'{data_dir}/gsim_discharge/'\n",
    "\n",
    "# define output folder\n",
    "fol_out = f'{out_dir}/discharge/'\n",
    "\n",
    "# run preprocess_gsim_discharge function (defined in f_preprocess_discharge.py) for all catchments in catch_id_list\n",
    "for catch_id in catch_id_list:\n",
    "    preprocess_gsim_discharge(catch_id, fol_in, fol_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c05b51e-4007-4864-8c3d-f443e9bf1365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Q</th>\n",
       "      <th>sd_mmd</th>\n",
       "      <th>iqr_mmd</th>\n",
       "      <th>min_mmd</th>\n",
       "      <th>max_mmd</th>\n",
       "      <th>min7_mmd</th>\n",
       "      <th>max7_mmd</th>\n",
       "      <th>doymin</th>\n",
       "      <th>doymax</th>\n",
       "      <th>doy7min</th>\n",
       "      <th>doy7max</th>\n",
       "      <th>nr_mis_days</th>\n",
       "      <th>nr_av_days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1964-12-31</td>\n",
       "      <td>2.782319</td>\n",
       "      <td>0.226279</td>\n",
       "      <td>0.264220</td>\n",
       "      <td>2.510092</td>\n",
       "      <td>3.170642</td>\n",
       "      <td>2.547837</td>\n",
       "      <td>3.114024</td>\n",
       "      <td>356.0</td>\n",
       "      <td>334.0</td>\n",
       "      <td>361.0</td>\n",
       "      <td>340.0</td>\n",
       "      <td>333.0</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1965-12-31</td>\n",
       "      <td>3.429440</td>\n",
       "      <td>1.101704</td>\n",
       "      <td>1.770275</td>\n",
       "      <td>1.587963</td>\n",
       "      <td>6.816881</td>\n",
       "      <td>1.587963</td>\n",
       "      <td>5.910983</td>\n",
       "      <td>356.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>362.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>365.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1966-12-31</td>\n",
       "      <td>2.563471</td>\n",
       "      <td>0.907277</td>\n",
       "      <td>1.030459</td>\n",
       "      <td>1.500771</td>\n",
       "      <td>14.400000</td>\n",
       "      <td>1.587963</td>\n",
       "      <td>4.642726</td>\n",
       "      <td>17.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>365.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1967-12-31</td>\n",
       "      <td>2.727136</td>\n",
       "      <td>0.985530</td>\n",
       "      <td>1.638165</td>\n",
       "      <td>1.004037</td>\n",
       "      <td>4.967339</td>\n",
       "      <td>1.004037</td>\n",
       "      <td>4.529489</td>\n",
       "      <td>281.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>365.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1968-12-31</td>\n",
       "      <td>2.812898</td>\n",
       "      <td>0.556125</td>\n",
       "      <td>0.930055</td>\n",
       "      <td>1.664587</td>\n",
       "      <td>3.989725</td>\n",
       "      <td>1.721206</td>\n",
       "      <td>3.566972</td>\n",
       "      <td>22.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>366.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date         Q    sd_mmd   iqr_mmd   min_mmd    max_mmd  min7_mmd  \\\n",
       "0  1964-12-31  2.782319  0.226279  0.264220  2.510092   3.170642  2.547837   \n",
       "1  1965-12-31  3.429440  1.101704  1.770275  1.587963   6.816881  1.587963   \n",
       "2  1966-12-31  2.563471  0.907277  1.030459  1.500771  14.400000  1.587963   \n",
       "3  1967-12-31  2.727136  0.985530  1.638165  1.004037   4.967339  1.004037   \n",
       "4  1968-12-31  2.812898  0.556125  0.930055  1.664587   3.989725  1.721206   \n",
       "\n",
       "   max7_mmd  doymin  doymax  doy7min  doy7max  nr_mis_days  nr_av_days  \n",
       "0  3.114024   356.0   334.0    361.0    340.0        333.0        33.0  \n",
       "1  5.910983   356.0   121.0    362.0    126.0          0.0       365.0  \n",
       "2  4.642726    17.0   159.0      1.0    165.0          0.0       365.0  \n",
       "3  4.529489   281.0   151.0    300.0    151.0          0.0       365.0  \n",
       "4  3.566972    22.0   175.0     26.0    176.0          0.0       366.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print output discharge dataframe for catchment [0]\n",
    "catch_id = catch_id_list[0]\n",
    "c = pd.read_csv(f'{fol_out}/timeseries/{catch_id}.csv')\n",
    "c.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f821fae5-8121-4375-bc8e-d5e03700abcb",
   "metadata": {},
   "source": [
    "### 4.2 Select catchments\n",
    "Here we select catchments based on the available data, following these criteria:\n",
    "- Timeseries should be longer than 10 years after 1980\n",
    "- Area quality given in GSIM database medium or high\n",
    "- Mean Q is not NaN\n",
    "- \n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ba0b3ba-a18f-434d-bd6b-7b9697849937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BR_0000495' 'FR_0000326' 'US_0002247' 'BR_0000208' 'CA_0002435'\n",
      " 'ZW_0000005']\n"
     ]
    }
   ],
   "source": [
    "s = select_catchments_q(out_dir)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2859eda9-f3b3-4c5c-a6cc-c7cdbb62732c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select catchments part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ffa5c8-eabd-4439-9018-bd5185d61a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% for selected catchments have a check on:\n",
    "    # - if amount of daily values in a year < 200 -> year=nan\n",
    "    # - amount of nan years in timeseries\n",
    "    # - if too many nan years -> exclude\n",
    "    \n",
    "# selected catchment names\n",
    "a = np.loadtxt(f'{out_dir}/catch_id_list_selected_q.txt',dtype=str) \n",
    "s = [] #list with new selected catchment names\n",
    "\n",
    "# loop over pre-selected catchments\n",
    "for i in range(len(a)):\n",
    "    b = pd.read_csv(f'{folder}GSIM_processed_data/catchment_yearly/mm_day/'+str(a[i])+'.csv',index_col=0)\n",
    "    b.index = pd.to_datetime(b.index)\n",
    "    \n",
    "    # select timeseries from 1980 onwards\n",
    "    b_end = b.index[-1]\n",
    "    if (b.index[0]<datetime.datetime(year=1980,month=1,day=1)):\n",
    "        b = b.loc['1980-12-31':str(b_end)]\n",
    "    \n",
    "    # change to nan if nr_av_days < 200\n",
    "    b.mean_mmd[b.nr_av_days < 200] = np.nan\n",
    "\n",
    "    # count amount of nan years\n",
    "    nan_sum = b.mean_mmd.isna().sum()\n",
    "    if nan_sum < 0.2*len(b):\n",
    "        s.append(a[i])\n",
    "    print(i)\n",
    "    \n",
    "    b.to_csv(f'{folder}GSIM_processed_data/catchment_selected_yearly/'+str(a[i])+'.csv')\n",
    "\n",
    "s = np.asarray(s)\n",
    "s = s.astype(str)\n",
    "np.savetxt(f'{folder}GSIM_processed_data/catch_id_selected2_mmd.txt',s,fmt = '%s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fe796c-118c-4e6c-8511-59be1b6a93c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7b0ae1-c42e-40dd-9b56-2bb56a4a670e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% read catch_id_selected\n",
    "a = np.loadtxt(f'{folder}GSIM_processed_data/catch_id_selected2_mmd.txt',dtype=str)   \n",
    "\n",
    "# convert upper case letters to lower case because shapefiles use lower case\n",
    "z = np.zeros(len(a),dtype='<U10')\n",
    "for i in range(len(z)):\n",
    "    k = a[i].astype(str)\n",
    "    k = k.lower()\n",
    "    z[i] = k\n",
    "\n",
    "np.savetxt(f'{folder}GSIM_processed_data/catch_id_selected2_mmd_lowercase.txt',z,fmt = '%s')\n",
    "\n",
    "\n",
    "#%% map the catchments with useful timeseries\n",
    "\n",
    "# append all shapefilenames of selected catchments\n",
    "shapefiles = []\n",
    "for i in range(len(z)):\n",
    "    shapefiles.append(str(folder)+'GSIM_data/GSIM_metadata/GSIM_catchments/'+str(z[i])+'.shp')\n",
    "\n",
    "\n",
    "shapefiles2=[]\n",
    "for i in range(len(shapefiles)):\n",
    "    myfile = Path(str(shapefiles[i]))\n",
    "    if myfile.is_file():\n",
    "        shapefiles2.append(myfile)\n",
    "        \n",
    "\n",
    "# concat shapefiles into one geodataframe  \n",
    "gdf = pd.concat([\n",
    "    gpd.read_file(shp)\n",
    "    for shp in shapefiles2\n",
    "]).pipe(gpd.GeoDataFrame)\n",
    "\n",
    "gdf.to_file(str(folder)+'GSIM_processed_data/merged_gpd_selected_catchments.shp')\n",
    "\n",
    "    \n",
    "#%% map gdf\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "gdf = gpd.read_file(str(folder)+'GSIM_processed_data/merged_gpd_selected_catchments.shp')\n",
    "\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "ax = fig.add_subplot(111)\n",
    "world.plot(ax=ax, edgecolor='black',linestyle=':', facecolor='white')\n",
    "\n",
    "gdf.plot(edgecolor='black',\n",
    "        linewidth=0.2,\n",
    "        ax=ax)\n",
    "ax.set_title('#catchments: '+str(len(gdf)),size=30)\n",
    "fig.savefig(str(folder)+'/GSIM_processed_data/figures/mapping_catchments.jpg',bbox_inches='tight')\n",
    "\n",
    "    \n",
    "    \n",
    "#%% add australia shapes to gdf\n",
    "gdf = gpd.read_file(str(folder)+'GSIM_processed_data/merged_gpd_selected_catchments.shp')\n",
    "aus = gpd.read_file('P:/Fransje/LSM root zone/CAMELS_AUS/02_location_boundary_area/shp/CAMELS_AUS_Boundaries_adopted.shp')\n",
    "gdf_aus = pd.concat([gdf,aus])    \n",
    "    \n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "ax = fig.add_subplot(111)\n",
    "world.plot(ax=ax, edgecolor='black',linestyle=':', facecolor='white')\n",
    "\n",
    "gdf_aus.plot(edgecolor='black',\n",
    "        linewidth=0.2,\n",
    "        ax=ax)\n",
    "ax.set_title('#catchments: '+str(len(gdf_aus)),size=30)\n",
    "fig.savefig(str(folder)+'/GSIM_processed_data/figures/mapping_catchments_aus.jpg',bbox_inches='tight')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4138fe53-5f10-4508-9e16-4b1c41b22cd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe425556-a09e-44d5-b874-b19bd91a73b7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0516da6d-192d-4f7d-b3c5-0ffe8db2f7a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1bfb49e0-e014-45eb-ac49-200b78d4c9b5",
   "metadata": {},
   "source": [
    "### 5. From gridded data to catchment timeseries\n",
    "For this step go to the notebook *run_script_grid_to_catchments*. This part is run in another notebook. The output data of this script can be found in *work_dir/output/forcing_timeseries*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7db8dd-39f3-4b02-93fc-2faaf74cb75b",
   "metadata": {},
   "source": [
    "### 6. Google earth engine for catchment characteristics\n",
    "For this step go to the notebook *run_script_earthengine*. This part is run in another notebook. The output data of this script can be found in *work_dir/output/earth_engine_timeseries*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecbdbeb-4659-482d-802d-dccd600b1658",
   "metadata": {},
   "source": [
    "### 7. Catchment descriptor variables\n",
    "For the global root zone storage capacity estimation, we need to calculate catchment descriptor variables. These descriptors can be climatological variables (e.g. mean precipitation (p_mean); seasonality of precipitation (si_p); timelag between maximum P and Ep (phi)) or landscape variables (e.g. mean treecover (tc); mean elevation (h_mean)). A detailed list of all the descriptors considered is provided here xxxxx.\\\n",
    "To calculate the catchment descriptor variables we use the *catch_characteristics* function from the *f_catch_characteristics.py* file. In this function you specify the variables of interest, the catchment ID and your in- and output folders. Then, based on all the timeseries you have generated in the preceding codes it will return a table with the catchment descriptor variables for all your catchments (that is saved as csv in your *work_dir/catchment_characteristics.csv*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ab7412-ed47-47a1-b5be-cff509d48929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define in and output folder\n",
    "fol_in=f'{work_dir}/output/'\n",
    "fol_out=f'{work_dir}/output/'\n",
    "\n",
    "# define variables of interest\n",
    "var=['p_mean','ep_mean','q_mean','t_mean','ai','si_p','si_ep','phi','tc','ntc','nonveg']\n",
    "\n",
    "# run catch_characteristics (defined in f_catch_characteristics.py) for the catchments in your catch_id_list\n",
    "catch_characteristics(var, catch_id_list, fol_in, fol_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95319679-461d-4b75-aca0-bbf9cfbf692c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine catchment geometries in single shapefile\n",
    "shape_dir = f'{data_dir}/shapes/'\n",
    "out_dir = f'{work_dir}/output'\n",
    "geo_catchments(shape_dir,out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f2a634-2971-4984-9de2-993faad4ac48",
   "metadata": {},
   "source": [
    "## check also for WB and for AREA (<10000km2) - see catchment_waterbalance.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f9e214-04ed-499e-9e45-845bc7d74c23",
   "metadata": {},
   "source": [
    "### 8. Calculate root zone storage capacity\n",
    "Here we calculate the catchment root zone storage capacity (Sr) based on catchment water balances. First, catchment root zone storage deficits (Sd) are computed as the cumulative difference between P and Et (transpiration). The result of one catchment is visualised in a figure. Second, the Sr is then calculated based on an extreme value analysis of the storage deficits for different return periods. A detailed description of this method can be found here xxxxxx.\n",
    "\n",
    "Here we use the *run_sd_calculation* and *run_sr_calculation* functions from the *f_sr_calculation* file. The Sd result of one catchment is visualised using *plot_sd*. The Sr results are merged using the *merge_sr* function and visualised using the *plot_sr* function. The output of both storage deficit and Sr calculations are saved in your *work_dir/output/sr_calculation*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409d935d-0bc8-473f-9018-a12a7af16576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make output directories\n",
    "if not os.path.exists(f'{work_dir}/output/sr_calculation/sd_catchments'):\n",
    "    os.makedirs(f'{work_dir}/output/sr_calculation/sd_catchments')\n",
    "    \n",
    "if not os.path.exists(f'{work_dir}/output/sr_calculation/sr_catchments'):\n",
    "    os.makedirs(f'{work_dir}/output/sr_calculation/sr_catchments')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4bed7a-fd38-422a-9577-e20cf6be7bac",
   "metadata": {},
   "source": [
    "Calculate storage deficits using the *run_sd_calculation* function from *f_sr_calculation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0641804-4e6a-4a1c-ad7d-fceaec17bc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define directories\n",
    "pep_dir = f'{work_dir}/output/forcing_timeseries/processed/daily'\n",
    "q_dir = f'{work_dir}/output/discharge/timeseries'\n",
    "out_dir = f'{work_dir}/output/sr_calculation/sd_catchments'\n",
    "\n",
    "# run sd calculation for all catchments in catch_id_list\n",
    "for catch_id in catch_id_list:\n",
    "    run_sd_calculation(catch_id, pep_dir, q_dir, out_dir)\n",
    "    \n",
    "#comRuud: print to screen what is being created (for some reason I did not get an Sd for the US catchment, but no error!)\n",
    "# Fransje: this is correct, incorrect water balance in us catchment -> no sd calculated. add flag or so in table??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84c3b66-6d7e-4081-bfb1-6cec197d70db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot sd example - use the first catchment from catch_id_list\n",
    "sd_dir = f'{work_dir}/output/sr_calculation/sd_catchments'\n",
    "plot_sd(catch_id_list[0], sd_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893710c3-63f6-482d-b348-9a434e79e787",
   "metadata": {},
   "source": [
    "Calculate Sr using the *run_sr_calculation* function from *f_sr_calculation* and merge the catchment Sr values into one dataframe with *merge_sr_catchments*. The functions return a table with the catchment Sr values for the different return periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882c1d56-675c-49a4-89e0-b1c5b40054c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define directories\n",
    "sd_dir = f'{work_dir}/output/sr_calculation/sd_catchments'\n",
    "out_dir = f'{work_dir}/output/sr_calculation/sr_catchments'\n",
    "\n",
    "# define return periods\n",
    "rp_array = [2,3,5,10,20,30,40,50,60]\n",
    "\n",
    "# run sr calculation for all catchments in catch_id_list\n",
    "for catch_id in catch_id_list:\n",
    "    run_sr_calculation(catch_id, rp_array, sd_dir, out_dir)\n",
    "    \n",
    "# merge catchment sr dataframes into one dataframe\n",
    "sr_dir = f'{work_dir}/output/sr_calculation/sr_catchments'\n",
    "out_dir = f'{work_dir}/output/sr_calculation/'\n",
    "merge_sr_catchments(sr_dir,out_dir)\n",
    "\n",
    "#comRuud: multiple Sr have been created (for different return periods? But values are the same, is that correct?) print to screen and tell what it should look like"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce395fd-fe07-444f-942c-6d2cae88823e",
   "metadata": {},
   "source": [
    "Mapping Sr using the *plot_sr* function from *f_sr_calculation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c841ffa-b51a-4e41-b34b-015b0c14572e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_file = f'{work_dir}/output/sr_calculation/sr_all_catchments.csv'\n",
    "shp_file = f'{work_dir}/output/geo_catchments.shp'\n",
    "rp=20\n",
    "plot_sr(shp_file,sr_file,rp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765ed214-c93f-40bd-bf5e-7e215dfc8066",
   "metadata": {},
   "source": [
    "### 9. Regression\n",
    "\n",
    "**move this to different script? to separate 'preprocessing' and 'analysis'?**\n",
    "\n",
    "Here we run the linear regression model to predict the catchment Sr values based on the descriptor parameters. We use the *f_regression* function to calculate the linear regression parameters for the considered catchments.\n",
    "We use the treecover data to separate the regression for high and low vegetation, the threshold values for tree cover (tc), non tree cover (ntc) and no-vegetation (nonveg) define this separation.\n",
    "\n",
    "The output is a figure showing the estimated (step 8) and predicted (from regression) Sr values and a table with the regression parameter values, some statistics for the regression performance and the threshold values for tree cover. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549b3ebf-5db9-4eda-9832-2439c8d960c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the catchment characteristics and sr tables\n",
    "cc_df = pd.read_csv(f'{work_dir}/output/catchment_characteristics.csv',index_col=0)\n",
    "sr_df = pd.read_csv(f'{work_dir}/output/sr_calculation/sr_all_catchments.csv',index_col=0)\n",
    "\n",
    "# define the descriptor variables\n",
    "dpar = ['p_mean','ep_mean','t_mean','si_p']\n",
    "\n",
    "# return period of Sr estimate\n",
    "rp = 20\n",
    "\n",
    "# define the vegetation thresholds for the regression\n",
    "tc_th, ntc_th, nonveg_th = 10, 0, 0\n",
    "\n",
    "# run the regression (r_regression in f_regression.py)\n",
    "run_regression(cc_df, sr_df, dpar, rp, tc_th, ntc_th, nonveg_th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3784f57-4033-469b-9191-7296fd3d89dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
