{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62e3f38f-c900-4727-a9e1-2735ae539100",
   "metadata": {},
   "source": [
    "# Main run script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9107eedc-c820-4eba-87b3-c52f84faf2fa",
   "metadata": {},
   "source": [
    "This script contains the main procedure to calculate global root zone storage capacities. \n",
    "\n",
    "This scripts only works in the conda environment **sr_env**. In this environment all required packages are available. If you have **not** installed and activated this environment before opening this script, you should check the installation section in the *README* file. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d86bf57-168c-4591-9050-c261b4adaafd",
   "metadata": {},
   "source": [
    "### 1. Getting started\n",
    "First, import all the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12476d9c-f4d9-41d7-889e-4e84ec7ee35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import calendar\n",
    "import geopandas as gpd\n",
    "import cartopy\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from pathos.threading import ThreadPool as Pool\n",
    "from scipy.optimize import least_squares\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import gaussian_kde\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from statsmodels.tools import add_constant\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d94080-a55a-49dc-bfd7-09ef86d1ffa2",
   "metadata": {},
   "source": [
    "Here we import all the python functions defined in the scripts *f_catch_characteristics.py* and *f_preprocess_discharge.py*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b053a7a-23b8-434d-ae1d-19888e5f5494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import python functions\n",
    "from f_catch_characteristics import *\n",
    "from f_preprocess_discharge import *\n",
    "from f_sr_calculation import *\n",
    "from f_regression import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a9ed88-646a-4209-b0a1-5e01426119ee",
   "metadata": {},
   "source": [
    "### 2. Define working and data directories\n",
    "Here we define the working directory, where all the scripts and output are saved.\n",
    "\n",
    "We also define the data directory where you have the following subdirectories:\n",
    "\n",
    "/data/forcing/*netcdf forcing files*\\\n",
    "/data/shapes/*catchment shapefiles*\\\n",
    "/data/gsim_discharge/*gsim discharge timeseries*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e8fa1bc-52f6-4907-ac52-d181690a37a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/u/LSM root zone/global_sr/scripts/global_sr_module'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check current working directory (helpful when filling in work_dir below)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48e094df-ec04-4c83-a259-9a4395300ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define your script working directory\n",
    "work_dir=Path(\"/mnt/u/LSM root zone/global_sr/\")\n",
    "# work_dir=Path('/tudelft.net/staff-umbrella/LSM root zone/global_sr')\n",
    "\n",
    "# define your data directory\n",
    "data_dir=Path(f'{work_dir}/data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e725c95a-41c4-41ee-8e26-a2d21cecd45b",
   "metadata": {},
   "source": [
    "Here we create the output directory inside your working directory. In the remainder of this module, the same command will be used regularly to create directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28501b38-7d4c-4c8d-9e0a-237aa5605dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make output directory\n",
    "if not os.path.exists(f'{work_dir}/output'):\n",
    "    os.makedirs(f'{work_dir}/output')\n",
    "out_dir = Path(f\"{work_dir}/output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e78deb3-3f08-44cc-b8d0-9b88e07afcad",
   "metadata": {},
   "source": [
    "### 2. GSIM discharge data\n",
    "### 2.1 Catchment id lists\n",
    "The GSIM discharge data contains yearly discharge files for ~30000 catchments. These files are stored here *{data_dir}/GSIM_data/GSIM_indices/TIMESERIES/yearly/* and origin from https://essd.copernicus.org/articles/10/765/2018/ and\n",
    "https://essd.copernicus.org/articles/10/787/2018/.\n",
    "Here we create lists of the catchment ids for later use, both with upper and lower characters. Files are saved in *out_dir/gsim/*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f9e0fa0d-eaa1-421c-ab69-c1f4e8c015c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of all catchment ids available in the GSIM yearly discharge timeseries data\n",
    "gsim_id_list_up = []\n",
    "gsim_id_list_lo = []\n",
    "for filepath in glob.iglob(f'{data_dir}/GSIM_data/GSIM_indices/TIMESERIES/yearly/*'):\n",
    "    f = os.path.split(filepath)[1] # remove full path\n",
    "    f = f[:-5] # remove .year extension\n",
    "    fl = f.lower()\n",
    "    gsim_id_list_up.append(f)\n",
    "    gsim_id_list_lo.append(fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4d2c34fb-33cc-4607-9c09-e725cd46d602",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f'{work_dir}/output/gsim'):\n",
    "    os.makedirs(f'{work_dir}/output/gsim')\n",
    "np.savetxt(f'{out_dir}/gsim/gsim_catch_id_list_up.txt',gsim_id_list_up,fmt='%s')\n",
    "np.savetxt(f'{out_dir}/gsim/gsim_catch_id_list_lo.txt',gsim_id_list_lo,fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5ad5a6-4a80-4876-b815-e22d0f4a1442",
   "metadata": {},
   "source": [
    "### 2.2 Preprocess data \n",
    "The GSIM yearly discharge timeseries are stored in *.year* files. A detailed explanation of the column names is provided in Table 3 and 4 in https://essd.copernicus.org/articles/10/787/2018/. Here we preprocess these data into readable *.csv* files for each catchment.\n",
    "\n",
    "The preprocessing function *preprocess_gsim_discharge* is defined in the file *f_preprocess_discharge.py*. With this function we generate for each catchment a file with the yearly discharge timeseries and a file with the specifications of the catchment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23ce5bd9-8f79-4a45-a598-04ddcb1d18fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make output directories\n",
    "if not os.path.exists(f'{out_dir}/gsim/timeseries'):\n",
    "    os.makedirs(f'{out_dir}/gsim/timeseries')\n",
    "\n",
    "if not os.path.exists(f'{out_dir}/gsim/timeseries_selected'):\n",
    "    os.makedirs(f'{out_dir}/gsim/timeseries_selected')\n",
    "    \n",
    "if not os.path.exists(f'{out_dir}/gsim/characteristics'):\n",
    "    os.makedirs(f'{out_dir}/gsim/characteristics')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e0041d-a369-47cd-afc7-69886d73cec6",
   "metadata": {},
   "source": [
    "Here we do a test run with only 5 catchments to reduce computational time. Run all catchments on Delftblue using slurm and the *run_gsim_preprocessing.py* script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b1495ea-6308-4d4d-ae77-8722d9683281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load catchment ids\n",
    "gsim_id_list_lo = np.loadtxt(f'{out_dir}/gsim/gsim_catch_id_list_lo.txt',dtype=str) \n",
    "\n",
    "# select randomly 5 catchments\n",
    "# gsim_id_list_lo = random.choices(gsim_id_list_lo,k=5)\n",
    "gsim_id_list_lo = gsim_id_list_lo[50:150]\n",
    "\n",
    "# define folder with discharge timeseries data\n",
    "fol_in = f'{data_dir}/GSIM_data/GSIM_indices/TIMESERIES/yearly/'\n",
    "\n",
    "# define output folder\n",
    "fol_out = f'{out_dir}/gsim/'\n",
    "\n",
    "# make lists for parallel computation\n",
    "catch_list = gsim_id_list_lo\n",
    "fol_in_list = [fol_in] * len(catch_list)\n",
    "fol_out_list = [fol_out] * len(catch_list)\n",
    "\n",
    "# run function\n",
    "run_function_parallel(catch_list,fol_in_list,fol_out_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83aacae4-7dfe-4ed1-9c51-0838801bdf7d",
   "metadata": {},
   "source": [
    "### 2.3 select GSIM catchments\n",
    "Here we select GSIM catchments that are used for further analysis. The selection criteria are as follows:\n",
    " - timeseries after 1980 contains at least 10 years of data\n",
    " - area quality high or medium\n",
    "\n",
    "If a year has less than 250 days with data, the year is set to nan, and we remove nan years from the timeseries.\n",
    "It is ok to have non-consecutive years in our timeseries.\n",
    "\n",
    "The function *select_catchments* in *f_preprocess_discharge.py* selects catchments and stores the selected timeseries in a separate folder.\n",
    "Here we do a test run with only 5 catchments to reduce computational time. Run all catchments on Delftblue using slurm and the *run_gsim_selection.py* script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77e7bc56-f1c7-4e69-b231-4cf94dbb0fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load catchment ids\n",
    "gsim_id_list_lo = np.loadtxt(f'{out_dir}/gsim/gsim_catch_id_list_lo.txt',dtype=str) \n",
    "\n",
    "# select randomly 5 catchments\n",
    "# gsim_id_list_lo = random.choices(gsim_id_list_lo,k=5)\n",
    "gsim_id_list_lo = gsim_id_list_lo[50:150]\n",
    "\n",
    "# make lists for parallel computation\n",
    "catch_list = gsim_id_list_lo\n",
    "data_dir_list = [data_dir] * len(catch_list)\n",
    "out_dir_list = [out_dir] * len(catch_list)\n",
    "\n",
    "# run function\n",
    "run_function2_parallel(data_dir_list,out_dir_list,catch_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd80a39-6bb2-4da1-b17d-33834b8b7080",
   "metadata": {},
   "source": [
    "Make here lists of the selected catchment ids, that are stored in the previous step in the *timeseries_selected* folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e29d5cdf-ac32-4225-bb40-b2198198e1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsim_id_list_up_sel = []\n",
    "gsim_id_list_lo_sel = []\n",
    "for filepath in glob.iglob(f'{out_dir}/gsim/timeseries_selected/*'):\n",
    "    f = os.path.split(filepath)[1] # remove full path\n",
    "    f = f[:-4] # remove .csv extension\n",
    "    fl = f.lower()\n",
    "    gsim_id_list_up_sel.append(f)\n",
    "    gsim_id_list_lo_sel.append(fl)\n",
    "    \n",
    "np.savetxt(f'{out_dir}/gsim/gsim_catch_id_list_up_sel.txt',gsim_id_list_up_sel,fmt='%s')\n",
    "np.savetxt(f'{out_dir}/gsim/gsim_catch_id_list_lo_sel.txt',gsim_id_list_lo_sel,fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea90e0c0",
   "metadata": {},
   "source": [
    "### 3. Organize data\n",
    "\n",
    "**Add Australia CAMELS catchments**\\\n",
    "The CAMELS-AU data is added to GSIM to get a better representation of climate regions. The discharge data is stored in /data/CAMELS_AUS/ and is processed using *preprocess_q_aus.py*\n",
    "Output is stored in /output/camels-aus/\n",
    "\n",
    "Here we assume all catchments are ok for Australia. \n",
    "- Copy all files in /output/camels_aus/ to /output/gsim/timeseries_selected/ (do manually) folder to complete the GSIM dataset, and move one folder up in output/\n",
    "- And we need to make a new list with selected catchments including australia - see below.\n",
    "- Copy all shapefiles of the selected GSIM and CAMELS-AUS catchments into /output/selected_shapes/\n",
    "- Copy all selected q_timeseries in /output/gsim/timeseries_selected to /output/q_timeseries_selected/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4efa4b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsim_id_aus_list_up_sel = []\n",
    "gsim_id_aus_list_lo_sel = []\n",
    "for filepath in glob.iglob(f'{out_dir}/q_timeseries_selected/*'):\n",
    "    f = os.path.split(filepath)[1] # remove full path\n",
    "    f = f[:-4] # remove .csv extension\n",
    "    fl = f.lower()\n",
    "    gsim_id_aus_list_up_sel.append(f)\n",
    "    gsim_id_aus_list_lo_sel.append(fl)\n",
    "    \n",
    "np.savetxt(f'{out_dir}/gsim_aus_catch_id_list_up_sel.txt',gsim_id_aus_list_up_sel,fmt='%s')\n",
    "np.savetxt(f'{out_dir}/gsim_aus_catch_id_list_lo_sel.txt',gsim_id_aus_list_lo_sel,fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f4f037a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['102101A' 'ar_0000001' 'ar_0000005' ... 'G9030124' 'G9030250' 'G9070142']\n",
      "The amount of selected catchments is: 8658\n"
     ]
    }
   ],
   "source": [
    "catch_list = np.loadtxt(f'{out_dir}/gsim_aus_catch_id_list_up_sel.txt',dtype=str)\n",
    "print(catch_list)\n",
    "print('The amount of selected catchments is:',len(catch_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfb49e0-e014-45eb-ac49-200b78d4c9b5",
   "metadata": {},
   "source": [
    "### 5. From gridded data to catchment timeseries\n",
    "For this step go to the notebook *run_script_grid_to_catchments*. This part is run in another notebook. The output data of this script can be found in:\n",
    "- */output/p_gswp_timeseries_selected_catchments*\n",
    "- */output/ep_gswp_timeseries_selected_catchments*\n",
    "- */output/tas_gswp_timeseries_selected_catchments*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b207de6-c57b-425f-b362-473cd1db1ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO HERE - POSTPROCESS FORCING TIMESERIES AND USE FORMAT FROM RUN_SCRIPT_GRID_TO_CATCHMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b643a5-9e5c-4ea9-92c6-f4bffc2c842c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b6baf9-088b-486d-b8da-fb37ee101c96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f7db8dd-39f3-4b02-93fc-2faaf74cb75b",
   "metadata": {},
   "source": [
    "### 6. Google earth engine for catchment characteristics\n",
    "For this step go to the notebook *run_script_earthengine*. This part is run in another notebook. The output data of this script can be found in *work_dir/output/earth_engine_timeseries*.\\\\\n",
    "\n",
    "- Treecover output is stored in */output/treecover/treecover_selected_shapes.csv*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecbdbeb-4659-482d-802d-dccd600b1658",
   "metadata": {},
   "source": [
    "### 7. Catchment descriptor variables\n",
    "For the global root zone storage capacity estimation, we need to calculate catchment descriptor variables. These descriptors can be climatological variables (e.g. mean precipitation (p_mean); seasonality of precipitation (si_p); timelag between maximum P and Ep (phi)) or landscape variables (e.g. mean treecover (tc); mean elevation (h_mean)). A detailed list of all the descriptors considered is provided here xxxxx.\\\n",
    "To calculate the catchment descriptor variables we use the *catch_characteristics* function from the *f_catch_characteristics.py* file. In this function you specify the variables of interest, the catchment ID and your in- and output folders. Then, based on all the timeseries you have generated in the preceding codes it will return a table with the catchment descriptor variables for all your catchments (that is saved as csv in your *work_dir/catchment_characteristics.csv*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ab7412-ed47-47a1-b5be-cff509d48929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define in and output folder\n",
    "fol_in=f'{work_dir}/output/'\n",
    "fol_out=f'{work_dir}/output/'\n",
    "\n",
    "# define variables of interest\n",
    "var=['p_mean','ep_mean','q_mean','t_mean','ai','si_p','si_ep','phi','tc','ntc','nonveg']\n",
    "\n",
    "# run catch_characteristics (defined in f_catch_characteristics.py) for the catchments in your catch_id_list\n",
    "catch_characteristics(var, catch_id_list, fol_in, fol_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95319679-461d-4b75-aca0-bbf9cfbf692c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine catchment geometries in single shapefile\n",
    "shape_dir = f'{data_dir}/shapes/'\n",
    "out_dir = f'{work_dir}/output'\n",
    "geo_catchments(shape_dir,out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f2a634-2971-4984-9de2-993faad4ac48",
   "metadata": {},
   "source": [
    "## check also for WB and for AREA (<10000km2) - see catchment_waterbalance.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f9e214-04ed-499e-9e45-845bc7d74c23",
   "metadata": {},
   "source": [
    "### 8. Calculate root zone storage capacity\n",
    "Here we calculate the catchment root zone storage capacity (Sr) based on catchment water balances. First, catchment root zone storage deficits (Sd) are computed as the cumulative difference between P and Et (transpiration). The result of one catchment is visualised in a figure. Second, the Sr is then calculated based on an extreme value analysis of the storage deficits for different return periods. A detailed description of this method can be found here xxxxxx.\n",
    "\n",
    "Here we use the *run_sd_calculation* and *run_sr_calculation* functions from the *f_sr_calculation* file. The Sd result of one catchment is visualised using *plot_sd*. The Sr results are merged using the *merge_sr* function and visualised using the *plot_sr* function. The output of both storage deficit and Sr calculations are saved in your *work_dir/output/sr_calculation*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409d935d-0bc8-473f-9018-a12a7af16576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make output directories\n",
    "if not os.path.exists(f'{work_dir}/output/sr_calculation/sd_catchments'):\n",
    "    os.makedirs(f'{work_dir}/output/sr_calculation/sd_catchments')\n",
    "    \n",
    "if not os.path.exists(f'{work_dir}/output/sr_calculation/sr_catchments'):\n",
    "    os.makedirs(f'{work_dir}/output/sr_calculation/sr_catchments')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4bed7a-fd38-422a-9577-e20cf6be7bac",
   "metadata": {},
   "source": [
    "Calculate storage deficits using the *run_sd_calculation* function from *f_sr_calculation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0641804-4e6a-4a1c-ad7d-fceaec17bc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define directories\n",
    "pep_dir = f'{work_dir}/output/forcing_timeseries/processed/daily'\n",
    "q_dir = f'{work_dir}/output/discharge/timeseries'\n",
    "out_dir = f'{work_dir}/output/sr_calculation/sd_catchments'\n",
    "\n",
    "# run sd calculation for all catchments in catch_id_list\n",
    "for catch_id in catch_id_list:\n",
    "    run_sd_calculation(catch_id, pep_dir, q_dir, out_dir)\n",
    "    \n",
    "#comRuud: print to screen what is being created (for some reason I did not get an Sd for the US catchment, but no error!)\n",
    "# Fransje: this is correct, incorrect water balance in us catchment -> no sd calculated. add flag or so in table??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84c3b66-6d7e-4081-bfb1-6cec197d70db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot sd example - use the first catchment from catch_id_list\n",
    "sd_dir = f'{work_dir}/output/sr_calculation/sd_catchments'\n",
    "plot_sd(catch_id_list[0], sd_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893710c3-63f6-482d-b348-9a434e79e787",
   "metadata": {},
   "source": [
    "Calculate Sr using the *run_sr_calculation* function from *f_sr_calculation* and merge the catchment Sr values into one dataframe with *merge_sr_catchments*. The functions return a table with the catchment Sr values for the different return periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882c1d56-675c-49a4-89e0-b1c5b40054c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define directories\n",
    "sd_dir = f'{work_dir}/output/sr_calculation/sd_catchments'\n",
    "out_dir = f'{work_dir}/output/sr_calculation/sr_catchments'\n",
    "\n",
    "# define return periods\n",
    "rp_array = [2,3,5,10,20,30,40,50,60]\n",
    "\n",
    "# run sr calculation for all catchments in catch_id_list\n",
    "for catch_id in catch_id_list:\n",
    "    run_sr_calculation(catch_id, rp_array, sd_dir, out_dir)\n",
    "    \n",
    "# merge catchment sr dataframes into one dataframe\n",
    "sr_dir = f'{work_dir}/output/sr_calculation/sr_catchments'\n",
    "out_dir = f'{work_dir}/output/sr_calculation/'\n",
    "merge_sr_catchments(sr_dir,out_dir)\n",
    "\n",
    "#comRuud: multiple Sr have been created (for different return periods? But values are the same, is that correct?) print to screen and tell what it should look like"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce395fd-fe07-444f-942c-6d2cae88823e",
   "metadata": {},
   "source": [
    "Mapping Sr using the *plot_sr* function from *f_sr_calculation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c841ffa-b51a-4e41-b34b-015b0c14572e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_file = f'{work_dir}/output/sr_calculation/sr_all_catchments.csv'\n",
    "shp_file = f'{work_dir}/output/geo_catchments.shp'\n",
    "rp=20\n",
    "plot_sr(shp_file,sr_file,rp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765ed214-c93f-40bd-bf5e-7e215dfc8066",
   "metadata": {},
   "source": [
    "### 9. Regression\n",
    "\n",
    "**move this to different script? to separate 'preprocessing' and 'analysis'?**\n",
    "\n",
    "Here we run the linear regression model to predict the catchment Sr values based on the descriptor parameters. We use the *f_regression* function to calculate the linear regression parameters for the considered catchments.\n",
    "We use the treecover data to separate the regression for high and low vegetation, the threshold values for tree cover (tc), non tree cover (ntc) and no-vegetation (nonveg) define this separation.\n",
    "\n",
    "The output is a figure showing the estimated (step 8) and predicted (from regression) Sr values and a table with the regression parameter values, some statistics for the regression performance and the threshold values for tree cover. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549b3ebf-5db9-4eda-9832-2439c8d960c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the catchment characteristics and sr tables\n",
    "cc_df = pd.read_csv(f'{work_dir}/output/catchment_characteristics.csv',index_col=0)\n",
    "sr_df = pd.read_csv(f'{work_dir}/output/sr_calculation/sr_all_catchments.csv',index_col=0)\n",
    "\n",
    "# define the descriptor variables\n",
    "dpar = ['p_mean','ep_mean','t_mean','si_p']\n",
    "\n",
    "# return period of Sr estimate\n",
    "rp = 20\n",
    "\n",
    "# define the vegetation thresholds for the regression\n",
    "tc_th, ntc_th, nonveg_th = 10, 0, 0\n",
    "\n",
    "# run the regression (r_regression in f_regression.py)\n",
    "run_regression(cc_df, sr_df, dpar, rp, tc_th, ntc_th, nonveg_th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3784f57-4033-469b-9191-7296fd3d89dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
