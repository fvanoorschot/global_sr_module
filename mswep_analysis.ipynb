{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b05b698-f8b3-4542-bc37-c0cbfd0683a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import scipy.stats\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import calendar\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import math\n",
    "import cartopy.crs as ccrs\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "from pathos.threading import ThreadPool as Pool\n",
    "from scipy.stats import gaussian_kde\n",
    "import xarray as xr\n",
    "from matplotlib import cm\n",
    "import matplotlib\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd3ae8de-a2cd-453c-9b8d-e38674ca765c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8217e790-c8b1-45b3-b8c1-c2b15a2bc89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from f_sr_calculation import *\n",
    "from f_snow_module import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a382f1b-e096-4dd6-b887-18773077d679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# work_dir=Path(\"/scratch/fransjevanoors/global_sr\")\n",
    "work_dir=Path(\"/mnt/u/LSM root zone/global_sr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55419460-b8f8-4538-afd7-0daac7f9c743",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44857807-7277-4f73-a805-7e834c63321b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_forcing_timeseries(catch_id,fol_in,fol_in2,fol_out,var):\n",
    "    \"\"\"\n",
    "    postprocess the catchment timeseries to usable pandas dataframes\n",
    "    catch_id:   str, catchment id\n",
    "    fol_in:     str, dir, folder with raw output csvs from grid-catchment extraction\n",
    "    fol_out:    str, dir, folder where to storre the processed dataframes\n",
    "    var:        str, list, list of variables calculated\n",
    "    \n",
    "    returns:\n",
    "    stores csvs of daily, monthly, yearly, climatology and mean timeseries    \n",
    "    \n",
    "    \"\"\"\n",
    "    # make empty dataframe\n",
    "    d = pd.DataFrame()\n",
    "\n",
    "    # for j in variable list - list the timeseries csvs for the catch id\n",
    "    l = glob.glob(fol_in + f\"*/{catch_id}*.csv\")\n",
    "    l2 = glob.glob(fol_in2 + f\"*/{catch_id}*.csv\")\n",
    "\n",
    "    l3 = []\n",
    "    for i in range(len(l)):\n",
    "        if ('gswp_p' in l[i]):\n",
    "            k=i\n",
    "        else:\n",
    "            l3.append(l[i])\n",
    "    l3.append(l2[0])\n",
    "\n",
    "    # combine variable timeseries in one dataframe\n",
    "    li=[] #make empty list\n",
    "    for filename in l3:\n",
    "        df = pd.read_csv(filename, index_col=0, header=0)\n",
    "        # df = df.drop(columns=['Unnamed: 0'])\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        df = df.loc['1981-01-01':'2010-12-31']\n",
    "        li.append(df) #append dataframe to list\n",
    "\n",
    "    d = pd.DataFrame()\n",
    "    frame = pd.concat(li, axis=1, ignore_index=False) #concatenate dataframes in li\n",
    "    col=frame.columns #get column names \n",
    "    y_start,y_end = frame.index[0].year, frame.index[-1].year #add columns with start and end years\n",
    "    d[col] = frame #add frame data to dataframe d\n",
    "    d = d.rename(columns={'Potential evaporation from GLEAM v3.5a':f'{var[0]}'}) #rename column names to variable list names\n",
    "    d = d.rename(columns={'precipitation':f'{var[1]}'})\n",
    "    d = d.rename(columns={'air_temperature':f'{var[2]}'})\n",
    "\n",
    "    # get daily timeseries and store as csv\n",
    "    if not os.path.exists(f'{fol_out}/daily'):\n",
    "         os.makedirs(f'{fol_out}/daily')\n",
    "    d.to_csv(f'{fol_out}/daily/{catch_id}_{y_start}_{y_end}.csv')\n",
    "\n",
    "    # get monthly timeseries and store as csv\n",
    "    if not os.path.exists(f'{fol_out}/monthly'):\n",
    "         os.makedirs(f'{fol_out}/monthly')\n",
    "    df_m = d.groupby(pd.Grouper(freq='M')).mean()\n",
    "    y_start,y_end = df_m.index[0].year, df_m.index[-1].year\n",
    "    df_m.to_csv(f'{fol_out}/monthly/{catch_id}_{y_start}_{y_end}.csv')    \n",
    "\n",
    "    # get climatology and store as csv\n",
    "    if not os.path.exists(f'{fol_out}/climatology'):\n",
    "         os.makedirs(f'{fol_out}/climatology')\n",
    "    df_m = df_m.groupby([df_m.index.month]).mean()\n",
    "    df_m.to_csv(f'{fol_out}/climatology/{catch_id}_{y_start}_{y_end}.csv')\n",
    "\n",
    "    # get yearly timeseries and store as csv\n",
    "    if not os.path.exists(f'{fol_out}/yearly'):\n",
    "         os.makedirs(f'{fol_out}/yearly')\n",
    "    df_y = d.groupby(pd.Grouper(freq='Y')).mean()\n",
    "    y_start,y_end = df_y.index[0].year, df_y.index[-1].year\n",
    "    df_y.to_csv(f'{fol_out}/yearly/{catch_id}_{y_start}_{y_end}.csv')\n",
    "\n",
    "    # get mean of timeseries and store as csv\n",
    "    if not os.path.exists(f'{fol_out}/mean'):\n",
    "         os.makedirs(f'{fol_out}/mean')\n",
    "    dm = d.mean()\n",
    "    dm.to_csv(f'{fol_out}/mean/{catch_id}_{y_start}_{y_end}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcbd4562-0ae3-44c1-bc77-69263deeb200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_processing_function_parallel(\n",
    "    catch_list=list,\n",
    "    fol_in_list=list,\n",
    "    fol_in2_list=list,\n",
    "    fol_out_list=list,\n",
    "    var_list=list,\n",
    "    threads=None\n",
    "    # threads=100\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs function preprocess_gsim_discharge  in parallel.\n",
    "​\n",
    "    catch_list:  str, list, list of catchmet ids\n",
    "    fol_in_list:     str, list, list of input folders\n",
    "    fol_out_list:   str, list, list of output folders\n",
    "    var_list: str,list, list of var list \n",
    "    threads:         int,       number of threads (cores), when set to None use all available threads\n",
    "​\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    # Set number of threads (cores) used for parallel run and map threads\n",
    "    if threads is None:\n",
    "        pool = Pool()\n",
    "    else:\n",
    "        pool = Pool(nodes=threads)\n",
    "    # Run parallel models\n",
    "    results = pool.map(\n",
    "        process_forcing_timeseries,\n",
    "        catch_list,\n",
    "        fol_in_list,\n",
    "        fol_in2_list,\n",
    "        fol_out_list,\n",
    "        var_list,\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2f16843e-34ae-455c-a003-535205e06bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_forcing_timeseries2(catch_id,fol_in,fol_in2,fol_out):\n",
    "    # make empty dataframe\n",
    "    d = pd.DataFrame()\n",
    "\n",
    "    # for j in variable list - list the timeseries csvs for the catch id\n",
    "    l = f'{fol_in}/{catch_id}_1981_2010.csv'\n",
    "    l2 = f'{fol_in2}/{catch_id}.csv'\n",
    "    \n",
    "    if ((os.path.exists(l)==True) and (os.path.exists(l2)==True)):\n",
    "        l1 = pd.read_csv(l,index_col=0)\n",
    "        l1.index = pd.to_datetime(l1.index)\n",
    "        l1 = l1.drop(columns='ep')\n",
    "        l2 = pd.read_csv(l2,index_col=0)\n",
    "        l2.index = pd.to_datetime(l2.index)\n",
    "        l2 = l2.drop(columns=['tas','tasmax','tasmin'])\n",
    "        l3 = pd.concat([l1,l2],axis=1)\n",
    "        l3 = l3.rename(columns={'ep_hs':'ep'})\n",
    "        d = l3\n",
    "\n",
    "        y_start,y_end = d.index[0].year, d.index[-1].year\n",
    "\n",
    "        # daily\n",
    "        d.to_csv(f'{fol_out}/daily/{catch_id}_{y_start}_{y_end}.csv')\n",
    "\n",
    "        # get monthly timeseries and store as csv\n",
    "        df_m = d.groupby(pd.Grouper(freq='M')).mean()\n",
    "        y_start,y_end = df_m.index[0].year, df_m.index[-1].year\n",
    "        df_m.to_csv(f'{fol_out}/monthly/{catch_id}_{y_start}_{y_end}.csv')    \n",
    "\n",
    "        # get climatology and store as csv\n",
    "        df_m = df_m.groupby([df_m.index.month]).mean()\n",
    "        df_m.to_csv(f'{fol_out}/climatology/{catch_id}_{y_start}_{y_end}.csv')\n",
    "\n",
    "        # get yearly timeseries and store as csv\n",
    "        df_y = d.groupby(pd.Grouper(freq='Y')).mean()\n",
    "        y_start,y_end = df_y.index[0].year, df_y.index[-1].year\n",
    "        df_y.to_csv(f'{fol_out}/yearly/{catch_id}_{y_start}_{y_end}.csv')\n",
    "\n",
    "        # get mean of timeseries and store as csv\n",
    "        dm = d.mean()\n",
    "        dm.to_csv(f'{fol_out}/mean/{catch_id}_{y_start}_{y_end}.csv')\n",
    "    \n",
    "def run_processing_function_parallel2(\n",
    "    catch_list=list,\n",
    "    fol_in_list=list,\n",
    "    fol_in2_list=list,\n",
    "    fol_out_list=list,\n",
    "    threads=None\n",
    "    # threads=100\n",
    "):\n",
    "    # Set number of threads (cores) used for parallel run and map threads\n",
    "    if threads is None:\n",
    "        pool = Pool()\n",
    "    else:\n",
    "        pool = Pool(nodes=threads)\n",
    "    # Run parallel models\n",
    "    results = pool.map(\n",
    "        process_forcing_timeseries2,\n",
    "        catch_list,\n",
    "        fol_in_list,\n",
    "        fol_in2_list,\n",
    "        fol_out_list,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1a197185-d912-460a-86c2-bc46b26a1c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add italy catchments to catch id list\n",
    "catch_id_list = np.genfromtxt(f'{work_dir}/output/gsim_aus_catch_id_list_lo_sel.txt',dtype='str')[:]\n",
    "\n",
    "it_list=[]\n",
    "for filepath in glob.iglob(f'{work_dir}/data/po_basin/organized_data/selected_shapes/*.shp'):\n",
    "    f = os.path.split(filepath)[1] # remove full path\n",
    "    f = f[:-4] # remove .year extension\n",
    "    it_list.append(f)\n",
    "it = np.array(it_list)\n",
    "catch_id_list2 = np.concatenate([catch_id_list,it])\n",
    "np.savetxt(f'{work_dir}/output/gsim_aus_catch_id_list_lo_sel_it.txt',catch_id_list2,fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e87448d0-e714-494f-8e03-958cae5a4385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ITEMI002', 'ITEMI073', 'ITEMI101', 'ITLOM122', 'ITLOM124',\n",
       "       'ITLOM128', 'ITLOM130', 'ITLOM132', 'ITLOM137', 'ITPIE168',\n",
       "       'ITPIE186', 'ITPIE189', 'ITPIE194', 'ITSAR226', 'ITSAR230',\n",
       "       'ITSAR233', 'ITSAR236', 'ITSAR237', 'ITTOS329', 'ITTOS331',\n",
       "       'ITTOS338', 'ITTOS339', 'ITTOS340', 'ITTOS343', 'ITTOS345',\n",
       "       'ITTOS349', 'ITTOS353', 'ITTOS372', 'ITTOS379', 'ITTOS384',\n",
       "       'ITTRE240', 'ITTRE243', 'ITTRE248', 'ITTRE262', 'ITTRE264',\n",
       "       'ITTRE267', 'ITUMB274', 'ITUMB275', 'ITUMB276', 'ITUMB277',\n",
       "       'ITUMB283', 'ITUMB285', 'ITVAL288', 'ITVEN427', 'ITVEN429',\n",
       "       'ITVEN430', 'ITVEN433', 'ITVEN434', 'ITVEN435'], dtype='<U8')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cb9c5065-5b91-4deb-8b60-8ed69bdfaf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p\n",
    "for catch_id in it:\n",
    "    # for j in variable list - list the timeseries csvs for the catch id\n",
    "    fol_in=f'{work_dir}/output/forcing_timeseries/raw'\n",
    "    l = glob.glob(fol_in + f\"*/{catch_id}*P*.csv\")\n",
    "\n",
    "    # combine variable timeseries in one dataframe\n",
    "    li=[] #make empty list\n",
    "    for filename in l:\n",
    "        df = pd.read_csv(filename, index_col=0, header=0)\n",
    "        # df = df.drop(columns=['Unnamed: 0'])\n",
    "        df.index = pd.to_datetime(df.time)\n",
    "        # df = df.loc['1981-01-01':'2010-12-31']\n",
    "        li.append(df) #append dataframe to list\n",
    "    d = pd.DataFrame()\n",
    "    frame = pd.concat(li, axis=0, ignore_index=False) #concatenate dataframes in li\n",
    "    frame = frame.drop(columns='time')\n",
    "    frame.to_csv(f'{fol_in}/{catch_id}_gswp_p_1981_2010.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4f8655ee-24e8-4e54-8fc1-c3e40410f977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tas\n",
    "for catch_id in it:\n",
    "    # for j in variable list - list the timeseries csvs for the catch id\n",
    "    fol_in=f'{work_dir}/output/forcing_timeseries/raw'\n",
    "    l = glob.glob(fol_in + f\"*/{catch_id}*T*.csv\")\n",
    "\n",
    "    # combine variable timeseries in one dataframe\n",
    "    li=[] #make empty list\n",
    "    for filename in l:\n",
    "        df = pd.read_csv(filename, index_col=0, header=0)\n",
    "        # df = df.drop(columns=['Unnamed: 0'])\n",
    "        df.index = pd.to_datetime(df.time)\n",
    "        # df = df.loc['1981-01-01':'2010-12-31']\n",
    "        li.append(df) #append dataframe to list\n",
    "    d = pd.DataFrame()\n",
    "    frame = pd.concat(li, axis=0, ignore_index=False) #concatenate dataframes in li\n",
    "    frame = frame.drop(columns='time')\n",
    "    frame.to_csv(f'{fol_in}/{catch_id}_gswp_tas_1981_2010.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "404629fa-c494-4dc0-b0d2-a1aa78fe5346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ep\n",
    "for catch_id in it[:]:\n",
    "    # for j in variable list - list the timeseries csvs for the catch id\n",
    "    fol_in=f'{work_dir}/output/forcing_timeseries/raw'\n",
    "    l = glob.glob(fol_in + f\"*/{catch_id}*Ep*.csv\")\n",
    "\n",
    "    # combine variable timeseries in one dataframe\n",
    "    li=[] #make empty list\n",
    "    for filename in l:\n",
    "        df = pd.read_csv(filename, index_col=0, header=0)\n",
    "        # df = df.drop(columns=['Unnamed: 0'])\n",
    "        df.index = pd.to_datetime(df.time)\n",
    "        # df = df.loc['1981-01-01':'2010-12-31']\n",
    "        li.append(df) #append dataframe to list\n",
    "    d = pd.DataFrame()\n",
    "    frame = pd.concat(li, axis=0, ignore_index=False) #concatenate dataframes in li\n",
    "    frame = frame.drop(columns='time')\n",
    "    frame.to_csv(f'{fol_in}/{catch_id}_gleam_ep_1980_2011.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a171b0-eecd-4b4e-99a4-212f701fb5e6",
   "metadata": {},
   "source": [
    "## processed timeseries with mswep p and gleam ep and gswp t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cbb8dff-83ae-4a3f-ad37-acd605a919cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# define input directory\n",
    "fol_in=f'{work_dir}/output/forcing_timeseries/raw'\n",
    "fol_in2=f'{work_dir}/output/forcing_timeseries/mswep_p/processed_timeseries'\n",
    "\n",
    "# define output directory\n",
    "fol_out=f'{work_dir}/output/forcing_timeseries/processed_mswep_gleam_gswp'\n",
    "\n",
    "# get catch_id_list\n",
    "catch_id_list = np.genfromtxt(f'{work_dir}/output/gsim_aus_catch_id_list_lo_sel_it.txt',dtype='str')[:]\n",
    "\n",
    "# check which catchments are missing\n",
    "el_id_list=[]\n",
    "for filepath in glob.iglob(f'{work_dir}/output/forcing_timeseries/processed_mswep_gleam_gswp/daily/*.csv'):\n",
    "    f = os.path.split(filepath)[1] # remove full path\n",
    "    f = f[:-14] # remove .year extension\n",
    "    el_id_list.append(f)\n",
    "dif = list(set(catch_id_list) - set(el_id_list))\n",
    "print(len(dif))\n",
    "catch_list = dif\n",
    "\n",
    "# define variables\n",
    "var = ['ep','p','tas']\n",
    "\n",
    "# run process_forcing_timeseries (defined in f_grid_to_catchments.py) for all catchments in catch_id_list\n",
    "# for catch_id in dif:\n",
    "#     process_forcing_timeseries(catch_id,fol_in,fol_in2,fol_out,var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6367f9de-4748-4461-a686-e547280ada28",
   "metadata": {},
   "outputs": [],
   "source": [
    "catch_id_list = dif[:]\n",
    "fol_in_list = [fol_in] * len(catch_id_list)\n",
    "fol_in2_list = [fol_in2] * len(catch_id_list)\n",
    "fol_out_list = [fol_out] * len(catch_id_list)\n",
    "var_list = [var] * len(catch_id_list)\n",
    "\n",
    "run_processing_function_parallel(catch_id_list,fol_in_list,fol_in2_list,fol_out_list,var_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b9ec581-4429-4308-b9d5-4a797f824280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/mnt/u/LSM root zone/global_sr/output/forcing_timeseries/processed_mswep_gleam_gswp/daily/102101A_1981_2010.csv']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ep</th>\n",
       "      <th>tas</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1981-01-01</th>\n",
       "      <td>4.744433</td>\n",
       "      <td>27.895155</td>\n",
       "      <td>2.787765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-02</th>\n",
       "      <td>4.475309</td>\n",
       "      <td>27.476841</td>\n",
       "      <td>1.442021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-03</th>\n",
       "      <td>6.261349</td>\n",
       "      <td>27.554550</td>\n",
       "      <td>47.797848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-04</th>\n",
       "      <td>4.613283</td>\n",
       "      <td>25.598467</td>\n",
       "      <td>16.896490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-05</th>\n",
       "      <td>3.863077</td>\n",
       "      <td>25.903513</td>\n",
       "      <td>6.125936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ep        tas          p\n",
       "time                                      \n",
       "1981-01-01  4.744433  27.895155   2.787765\n",
       "1981-01-02  4.475309  27.476841   1.442021\n",
       "1981-01-03  6.261349  27.554550  47.797848\n",
       "1981-01-04  4.613283  25.598467  16.896490\n",
       "1981-01-05  3.863077  25.903513   6.125936"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print P Ep T timeseries for catchment [0] in catch_id_list\n",
    "catch_id = catch_id_list[0]\n",
    "f = glob.glob(f'{fol_out}/daily/{catch_id}*.csv')\n",
    "print(f)\n",
    "c = pd.read_csv(f[0], index_col=0)\n",
    "c.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f862e6f-e4bf-401a-97cc-3c60e9c3995d",
   "metadata": {},
   "source": [
    "## processed timeseries with mswep p and hargreaves ep and gswp t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ba25e4c1-8e66-4d48-995f-e4befbb28a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# define input directory\n",
    "fol_in=f'{work_dir}/output/forcing_timeseries/processed_mswep_gleam_gswp/daily'\n",
    "fol_in2=f'{work_dir}/output/forcing_timeseries/ep_hargreaves'\n",
    "\n",
    "# define output directory\n",
    "fol_out=f'{work_dir}/output/forcing_timeseries/processed_mswep_hs_gswp'\n",
    "\n",
    "# get catch_id_list\n",
    "catch_id_list = np.genfromtxt(f'{work_dir}/output/gsim_aus_catch_id_list_lo_sel_it.txt',dtype='str')[:]\n",
    "\n",
    "# check which catchments are missing\n",
    "el_id_list=[]\n",
    "for filepath in glob.iglob(f'{work_dir}/output/forcing_timeseries/processed_mswep_hs_gswp/daily/*.csv'):\n",
    "    f = os.path.split(filepath)[1] # remove full path\n",
    "    f = f[:-14] # remove .year extension\n",
    "    el_id_list.append(f)\n",
    "dif = list(set(catch_id_list) - set(el_id_list))\n",
    "print(len(dif))\n",
    "catch_list = dif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "72de2f0c-cb07-4d2b-81b6-d7d23e18aaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "catch_id_list = dif[:]\n",
    "fol_in_list = [fol_in] * len(catch_id_list)\n",
    "fol_in2_list = [fol_in2] * len(catch_id_list)\n",
    "fol_out_list = [fol_out] * len(catch_id_list)\n",
    "\n",
    "run_processing_function_parallel2(catch_id_list,fol_in_list,fol_in2_list,fol_out_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8312c2a7-6bec-481a-8dc5-72a793438eb4",
   "metadata": {},
   "source": [
    "## processed timeseries with gswp p and hargreaves ep and gswp t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d9364146-e626-4692-8434-faa9b65b1443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# define input directory\n",
    "fol_in=f'{work_dir}/output/forcing_timeseries/processed/daily'\n",
    "fol_in2=f'{work_dir}/output/forcing_timeseries/ep_hargreaves'\n",
    "\n",
    "# define output directory\n",
    "fol_out=f'{work_dir}/output/forcing_timeseries/processed_gswp_hs_gswp'\n",
    "\n",
    "# get catch_id_list\n",
    "catch_id_list = np.genfromtxt(f'{work_dir}/output/gsim_aus_catch_id_list_lo_sel_it.txt',dtype='str')[:]\n",
    "\n",
    "# check which catchments are missing\n",
    "el_id_list=[]\n",
    "for filepath in glob.iglob(f'{work_dir}/output/forcing_timeseries/processed_gswp_hs_gswp/daily/*.csv'):\n",
    "    f = os.path.split(filepath)[1] # remove full path\n",
    "    f = f[:-14] # remove .year extension\n",
    "    el_id_list.append(f)\n",
    "dif = list(set(catch_id_list) - set(el_id_list))\n",
    "print(len(dif))\n",
    "catch_list = dif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "141af57b-05f2-48c6-9578-b6a7330925e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "catch_id_list = dif[:]\n",
    "fol_in_list = [fol_in] * len(catch_id_list)\n",
    "fol_in2_list = [fol_in2] * len(catch_id_list)\n",
    "fol_out_list = [fol_out] * len(catch_id_list)\n",
    "\n",
    "run_processing_function_parallel2(catch_id_list,fol_in_list,fol_in2_list,fol_out_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3deef24-87e7-4eb7-9cc8-212085850bd3",
   "metadata": {},
   "source": [
    "## Run snow calculation with mswep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c926691f-ec26-4eca-8bf9-281d9f5e96a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select catchments with temperature <0 AND precipitation>1, because if no p there is no snow possible\n",
    "catch_id_list = np.genfromtxt(f'{work_dir}/output/gsim_aus_catch_id_list_lo_sel_it.txt',dtype='str')[:]\n",
    "snow_catch=[]\n",
    "for catch_id in catch_id_list:\n",
    "    a = pd.read_csv(f'{work_dir}/output/forcing_timeseries/processed_mswep_gleam_gswp/daily/{catch_id}_1981_2010.csv',index_col=0)\n",
    "    t = a[(a.tas<0)&(a.p>1)]\n",
    "    if (len(t))>0:\n",
    "        if (len(t)>0.05*len(a)):\n",
    "            snow_catch.append(catch_id)\n",
    "np.savetxt(f'{work_dir}/output/snow/catch_id_list_snow_t_and_p_mswep.txt',snow_catch,fmt='%s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b33df1a-9fdd-437e-92f4-01e9416733ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2571"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snow_id_list = np.genfromtxt(f'{work_dir}/output/snow/catch_id_list_snow_t_and_p_mswep.txt',dtype='str')[:]\n",
    "len(snow_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a2e8e54-ba34-4b97-82b2-8a006b9df776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "catch_id_list = np.genfromtxt(f'{work_dir}/output/snow/catch_id_list_snow_t_and_p_mswep.txt',dtype='str')\n",
    "\n",
    "# check which catchments are missing\n",
    "el_id_list=[]\n",
    "for filepath in glob.iglob(f'{work_dir}/output/snow/timeseries_mswep/*.csv'):\n",
    "    f = os.path.split(filepath)[1] # remove full path\n",
    "    f = f[:-4] # remove .year extension\n",
    "    el_id_list.append(f)\n",
    "dif = list(set(catch_id_list) - set(el_id_list))\n",
    "print(len(dif))\n",
    "catch_list = dif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc93853f-08bf-45c4-a027-e4c1b3ab226b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in parallel\n",
    "# make lists for parallel computation\n",
    "catch_list = catch_list[:]\n",
    "work_dir_list = [work_dir] * len(catch_list)\n",
    "run_function_parallel_snow_mswep(catch_list, work_dir_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4556a1-843f-45ef-872c-27488dbe8fa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04f3e40-1d56-4d53-8f56-7d3ef51172ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ec19597-2530-4dc0-b55b-b398f1864e00",
   "metadata": {},
   "source": [
    "## Run Sd calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1e4d441e-0e80-4096-a591-8c17ae74a876",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_584/3758570081.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mir_case_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mir_case\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcatch_id_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mrun_sd_calculation_parallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcatch_id_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpep_dir_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq_dir_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mout_dir_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msnow_id_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msnow_dir_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwork_dir_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mir_case_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/global_sr_module/f_sr_calculation.py\u001b[0m in \u001b[0;36mrun_sd_calculation_parallel\u001b[0;34m(catch_id_list, pep_dir_list, q_dir_list, out_dir_list, snow_id_list, snow_dir_list, work_dir_list, ir_case_list, threads)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthreads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[0;31m# Run parallel models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m     results = pool.map(\n\u001b[0m\u001b[1;32m    499\u001b[0m         \u001b[0mrun_sd_calculation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0mcatch_id_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/sr_env/lib/python3.9/site-packages/pathos/threading.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, f, *args, **kwds)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mAbstractWorkerPool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_AbstractWorkerPool__map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0m_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_serve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# chunksize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m     \u001b[0mmap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAbstractWorkerPool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mimap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/sr_env/lib/python3.9/site-packages/multiprocess/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         '''\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/sr_env/lib/python3.9/site-packages/multiprocess/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/sr_env/lib/python3.9/site-packages/multiprocess/pool.py\u001b[0m in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwrap_exception\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_helper_reraises_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/sr_env/lib/python3.9/site-packages/multiprocess/pool.py\u001b[0m in \u001b[0;36mmapstar\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstarmapstar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/sr_env/lib/python3.9/site-packages/pathos/helpers/mp_helper.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstarargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;34m\"\"\"decorator to convert a many-arg function to a single-arg function\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m    \u001b[0;31m#func.__module__ = f.__module__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m    \u001b[0;31m#func.__name__ = f.__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/global_sr_module/f_sr_calculation.py\u001b[0m in \u001b[0;36mrun_sd_calculation\u001b[0;34m(catch_id, pep_dir, q_dir, out_dir, snow_id_list, snow_dir, work_dir, ir_case)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0mq_ts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_q\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0mq_ts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_ts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m     \u001b[0mpep_ts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_pep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m     \u001b[0mpep_ts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpep_ts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "ir_case = 'iaf' # or 'ni' or 'iwu' or 'iaf'\n",
    "pep_dir = f'{work_dir}/output/forcing_timeseries/processed_mswep_gleam_gswp/daily'\n",
    "q_dir = f'{work_dir}/output/q_timeseries_selected'\n",
    "out_dir = f'{work_dir}/output/sr_calculation/sd_catchments_mswep_gleam'\n",
    "snow_ids = np.genfromtxt(f'{work_dir}/output/snow/catch_id_list_snow_t_and_p.txt',dtype='str')\n",
    "snow_dir = f'{work_dir}/output/snow/timeseries_mswep'\n",
    "\n",
    "catch_list = np.genfromtxt(f'{work_dir}/output/gsim_aus_catch_id_list_lo_sel_area_wb.txt',dtype='str')[:]\n",
    "# itlist = np.genfromtxt(f'{work_dir}/data/po_basin/organized_data/it_selected_catchments.txt',dtype='str')\n",
    "# catch_list = itlist\n",
    "\n",
    "# # check which catchments are missing\n",
    "# el_id_list=[]\n",
    "# for filepath in glob.iglob(f'{work_dir}/output/sr_calculation/sd_catchments_mswep_gleam/irri/f0.9ia/sd/*.csv'):\n",
    "#     f = os.path.split(filepath)[1] # remove full path\n",
    "#     f = f[:-11] # remove .year extension\n",
    "#     el_id_list.append(f)\n",
    "# dif = list(set(catch_list) - set(el_id_list))\n",
    "# print(len(dif))\n",
    "# catch_list = dif\n",
    "\n",
    "catch_list = ['ca_0004894']#,'fr_0000874']\n",
    "\n",
    "catch_id_list = catch_list\n",
    "pep_dir_list = [pep_dir] * len(catch_id_list)\n",
    "q_dir_list = [q_dir] * len(catch_id_list)\n",
    "out_dir_list = [out_dir] * len(catch_id_list)\n",
    "snow_id_list = [snow_ids] * len(catch_id_list)\n",
    "snow_dir_list = [snow_dir] * len(catch_id_list)\n",
    "work_dir_list = [work_dir] * len(catch_id_list)\n",
    "ir_case_list = [ir_case] * len(catch_id_list)\n",
    "\n",
    "run_sd_calculation_parallel(catch_id_list,pep_dir_list,q_dir_list,out_dir_list,snow_id_list,snow_dir_list,work_dir_list,ir_case_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3c926152-0fd5-4f31-91b1-8836c61d802b",
   "metadata": {},
   "outputs": [],
   "source": [
    "catch_id = 'ca_0004894'\n",
    "snow_id_list = snow_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2ef1112d-e492-4c34-be16-ab462e59d64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if catch_id in snow_id_list:\n",
    "#     s = 1 # snow is yes\n",
    "#     f_pep = glob.glob(f'{snow_dir}/{catch_id}*.csv')\n",
    "\n",
    "# else:\n",
    "#     s = 0 # snow is no\n",
    "#     # get P Ep and Q files for catch id\n",
    "f_pep = glob.glob(f'{pep_dir}/{catch_id}*.csv')\n",
    "\n",
    "cc = pd.read_csv(f'{work_dir}/output/catchment_characteristics/gswp-p_gleam-ep_gswp-t/landscape/{catch_id}.csv',index_col=0)\n",
    "ir_area = cc.ia.values\n",
    "\n",
    "# read q df\n",
    "f_q = glob.glob(f'{q_dir}/{catch_id}*.csv')\n",
    "\n",
    "# read files as dataframes\n",
    "q_ts = pd.read_csv(f_q[0],index_col=0)\n",
    "q_ts.index = pd.to_datetime(q_ts.index)\n",
    "pep_ts = pd.read_csv(f_pep[0],index_col=0)\n",
    "pep_ts.index = pd.to_datetime(pep_ts.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f9be4e79-4fb2-40dd-89e4-68c1b42f65e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ep</th>\n",
       "      <th>tas</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1981-01-01</th>\n",
       "      <td>0.095865</td>\n",
       "      <td>-3.937836</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-02</th>\n",
       "      <td>0.022581</td>\n",
       "      <td>-5.562408</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-03</th>\n",
       "      <td>0.036574</td>\n",
       "      <td>-3.593994</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-04</th>\n",
       "      <td>0.082080</td>\n",
       "      <td>-3.011627</td>\n",
       "      <td>0.158732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-05</th>\n",
       "      <td>0.033199</td>\n",
       "      <td>-2.026978</td>\n",
       "      <td>2.009772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-12-27</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.407257</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-12-28</th>\n",
       "      <td>0.039419</td>\n",
       "      <td>-5.179138</td>\n",
       "      <td>0.632407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-12-29</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-9.887177</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-12-30</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-16.543243</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-12-31</th>\n",
       "      <td>0.022488</td>\n",
       "      <td>-17.868973</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10957 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ep        tas         p\n",
       "time                                     \n",
       "1981-01-01  0.095865  -3.937836  0.000000\n",
       "1981-01-02  0.022581  -5.562408  0.000000\n",
       "1981-01-03  0.036574  -3.593994  0.000000\n",
       "1981-01-04  0.082080  -3.011627  0.158732\n",
       "1981-01-05  0.033199  -2.026978  2.009772\n",
       "...              ...        ...       ...\n",
       "2010-12-27  0.000000  -3.407257  0.000000\n",
       "2010-12-28  0.039419  -5.179138  0.632407\n",
       "2010-12-29  0.000000  -9.887177  0.000000\n",
       "2010-12-30  0.000000 -16.543243  0.000000\n",
       "2010-12-31  0.022488 -17.868973  0.000000\n",
       "\n",
       "[10957 rows x 3 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pep_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee901d9-1a16-4a81-b9f2-b23613154a47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8e5a06f-4793-4d54-8185-51f13e3528b3",
   "metadata": {},
   "source": [
    "## SR CALCULATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5ec7b39-8221-4d15-bf21-62d61e344911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4514"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd_list2=[]\n",
    "for filepath in glob.iglob(f'{work_dir}/output/sr_calculation/sd_catchments_mswep_gleam/irri/f0.9ia/sr/*gumbelfit.csv'):\n",
    "    f = os.path.split(filepath)[1] # remove full path\n",
    "    f = f[:-21] \n",
    "    sd_list2.append(f)\n",
    "len(sd_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb3413d0-97df-4814-a67f-8895535f20ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dif = list(set(sd_list2) - set(sd_list1))\n",
    "len(dif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269ee79c-0596-477f-8c56-a1eea5ff4b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ir_case = 'iaf' #'ni' or iwu or iaf\n",
    "catch_list = np.genfromtxt(f'{work_dir}/output/gsim_aus_catch_id_list_lo_sel_area_wb.txt',dtype='str')[:]\n",
    "# itlist = np.genfromtxt(f'{work_dir}/data/po_basin/organized_data/it_selected_catchments.txt',dtype='str')\n",
    "\n",
    "sd_list=[]\n",
    "for filepath in glob.iglob(f'{work_dir}/output/sr_calculation/sd_catchments_mswep_gleam/irri/f0.9ia/sd/*.csv'):\n",
    "    f = os.path.split(filepath)[1] # remove full path\n",
    "    f = f[:-11] \n",
    "    sd_list.append(f)\n",
    "\n",
    "# define directories\n",
    "sd_dir = f'{work_dir}/output/sr_calculation/sd_catchments'\n",
    "out_dir = f'{work_dir}/output/sr_calculation/sd_catchments'\n",
    "\n",
    "# define return periods\n",
    "rp_array = [1.5,2,3,5,10,20,30,40,50,60,70,80]\n",
    "catch_id_list = dif\n",
    "sd_dir_list = [sd_dir] * len(catch_id_list)\n",
    "out_dir_list = [out_dir] * len(catch_id_list) \n",
    "rp_array_list = [rp_array] * len(catch_id_list) \n",
    "ir_case_list = [ir_case] * len(catch_id_list)\n",
    "\n",
    "run_sr_calculation_parallel(catch_id_list,rp_array_list,sd_dir_list,out_dir_list,ir_case_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58d1a2d-a5f8-455d-b9e0-d409bc10d32b",
   "metadata": {},
   "source": [
    "Combine calculated Sr values in one dataframe for each irrigation case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d39192e8-5a3b-4031-ab88-818e712eb1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4513\n"
     ]
    }
   ],
   "source": [
    "# combine Sr output in dataframe - f0.9ia\n",
    "c_list=[]\n",
    "for filepath in glob.iglob(f'{work_dir}/output/sr_calculation/sd_catchments_mswep_gleam/irri/f0.9ia/sr_rzyear/*gumbelfit.csv'):\n",
    "    f = os.path.split(filepath)[1] # remove full path\n",
    "    f = f[:-21] # remove .year extension\n",
    "    c_list.append(f)\n",
    "print(len(c_list))\n",
    "\n",
    "sr_df_gf = pd.DataFrame(index=c_list, columns=['1.5','2.0','3.0','5.0','10.0','20.0','30.0','40.0','50.0','60.0','70.0','80.0'])\n",
    "sr_df_p = pd.DataFrame(index=c_list, columns=['k3','k5'])\n",
    "# sr_df_gf.to_csv(f'{work_dir}/output/sr_calculation/sd_catchments/irri/f0.9ia/sr_irri_f0.9ia_combined_gumbelfit2_rzyear.csv')\n",
    "# sr_df_p.to_csv(f'{work_dir}/output/sr_calculation/sd_catchments/irri/f0.9ia/sr_irri_f0.9ia_combined_points2_rzyear.csv')\n",
    "\n",
    "# sr_df_gf = pd.read_csv(f'{work_dir}/output/sr_calculation/sd_catchments/irri/f0.9ia/sr_irri_f0.9ia_combined_gumbelfit_rzyear.csv',index_col=0)\n",
    "# sr_df_p = pd.read_csv(f'{work_dir}/output/sr_calculation/sd_catchments/irri/f0.9ia/sr_irri_f0.9ia_combined_points_rzyear.csv',index_col=0)\n",
    "# sr_n = sr_df_p.dropna()\n",
    "\n",
    "p=[]\n",
    "# p = sr_n.index\n",
    "di = list(set(c_list) - set(p)) #missing catchments in dataframe\n",
    "\n",
    "for catch_id in di[:]:\n",
    "    if os.path.exists(f'{work_dir}/output/sr_calculation/sd_catchments_mswep_gleam/irri/f0.9ia/sr_rzyear/{catch_id}_f0.9ia_gumbelfit.csv'):\n",
    "        d = pd.read_csv(f'{work_dir}/output/sr_calculation/sd_catchments_mswep_gleam/irri/f0.9ia/sr_rzyear/{catch_id}_f0.9ia_gumbelfit.csv',index_col=0)\n",
    "        sr_df_gf.loc[catch_id] = d.iloc[0]\n",
    "        \n",
    "        d = pd.read_csv(f'{work_dir}/output/sr_calculation/sd_catchments_mswep_gleam/irri/f0.9ia/sr_rzyear/{catch_id}_f0.9ia_points.csv',index_col=0)\n",
    "        l = [2]#,3,5,10,20,30,40,50,60,70,80]\n",
    "        for i in range(len(l)):\n",
    "            a = d['T_a'].values\n",
    "            ix1 = find_nearest(a,l[i])\n",
    "            sr = np.mean(d.sd.values[[ix1,ix1-1,ix1+1]])\n",
    "            sr_df_p.loc[catch_id]['k3'] = sr\n",
    "            \n",
    "            a = d['T_a'].values\n",
    "            ix1 = find_nearest(a,l[i])\n",
    "            sr = np.mean(d.sd.values[[ix1,ix1-1,ix1+1,ix1-2,ix1+2]])\n",
    "            sr_df_p.loc[catch_id]['k5'] = sr\n",
    "\n",
    "sr_df_gf.to_csv(f'{work_dir}/output/sr_calculation/sd_catchments_mswep_gleam/irri/f0.9ia/sr_irri_f0.9ia_combined_gumbelfit2_rzyear.csv')\n",
    "sr_df_p.to_csv(f'{work_dir}/output/sr_calculation/sd_catchments_mswep_gleam/irri/f0.9ia/sr_irri_f0.9ia_combined_points2_rzyear.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb35d7e-56e2-40fd-8f16-fb7fec89f537",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0babee95-80af-45cc-9e16-5394a3f0f0e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
