{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62e3f38f-c900-4727-a9e1-2735ae539100",
   "metadata": {},
   "source": [
    "# Main run script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9107eedc-c820-4eba-87b3-c52f84faf2fa",
   "metadata": {},
   "source": [
    "This script contains the main procedure to calculate global root zone storage capacities. \n",
    "\n",
    "This scripts only works in the conda environment **sr_env**. In this environment all required packages are available. If you have **not** installed and activated this environment before opening this script, you should check the installation section in the *README* file. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d86bf57-168c-4591-9050-c261b4adaafd",
   "metadata": {},
   "source": [
    "### 1. Getting started\n",
    "First, import all the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12476d9c-f4d9-41d7-889e-4e84ec7ee35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import calendar\n",
    "import geopandas as gpd\n",
    "import cartopy\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from pathos.threading import ThreadPool as Pool\n",
    "from scipy.optimize import least_squares\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import gaussian_kde\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from statsmodels.tools import add_constant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d94080-a55a-49dc-bfd7-09ef86d1ffa2",
   "metadata": {},
   "source": [
    "Here we import all the python functions defined in the scripts *f_catch_characteristics.py* and *f_preprocess_discharge.py*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b053a7a-23b8-434d-ae1d-19888e5f5494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import python functions\n",
    "from f_catch_characteristics import *\n",
    "from f_preprocess_discharge import *\n",
    "from f_sr_calculation import *\n",
    "from f_regression import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a9ed88-646a-4209-b0a1-5e01426119ee",
   "metadata": {},
   "source": [
    "### 2. Define working and data directories\n",
    "Here we define the working directory, where all the scripts and output are saved.\n",
    "\n",
    "We also define the data directory where you have the following subdirectories:\n",
    "\n",
    "/data/forcing/*netcdf forcing files*\\\n",
    "/data/shapes/*catchment shapefiles*\\\n",
    "/data/gsim_discharge/*gsim discharge timeseries*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e8fa1bc-52f6-4907-ac52-d181690a37a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tudelft.net/staff-umbrella/LSM root zone/global_sr/scripts/global_sr_module'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check current working directory (helpful when filling in work_dir below)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48e094df-ec04-4c83-a259-9a4395300ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define your script working directory\n",
    "# work_dir=Path(\"/mnt/u/LSM root zone/global_sr/\")\n",
    "work_dir=Path('/tudelft.net/staff-umbrella/LSM root zone/global_sr')\n",
    "\n",
    "# define your data directory\n",
    "data_dir=Path(f'{work_dir}/data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e725c95a-41c4-41ee-8e26-a2d21cecd45b",
   "metadata": {},
   "source": [
    "Here we create the output directory inside your working directory. In the remainder of this module, the same command will be used regularly to create directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28501b38-7d4c-4c8d-9e0a-237aa5605dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make output directory\n",
    "if not os.path.exists(f'{work_dir}/output'):\n",
    "    os.makedirs(f'{work_dir}/output')\n",
    "out_dir = Path(f\"{work_dir}/output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e78deb3-3f08-44cc-b8d0-9b88e07afcad",
   "metadata": {},
   "source": [
    "### 2. GSIM discharge data\n",
    "### 2.1 Catchment id lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f9e0fa0d-eaa1-421c-ab69-c1f4e8c015c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of all catchment ids available in the GSIM yearly discharge timeseries data\n",
    "gsim_id_list_up = []\n",
    "gsim_id_list_lo = []\n",
    "for filepath in glob.iglob(f'{data_dir}/GSIM_data/GSIM_indices/TIMESERIES/yearly/*'):\n",
    "    f = os.path.split(filepath)[1] # remove full path\n",
    "    f = f[:-5] # remove .year extension\n",
    "    fl = f.lower()\n",
    "    gsim_id_list_up.append(f)\n",
    "    gsim_id_list_lo.append(fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4d2c34fb-33cc-4607-9c09-e725cd46d602",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f'{work_dir}/output/gsim'):\n",
    "    os.makedirs(f'{work_dir}/output/gsim')\n",
    "np.savetxt(f'{out_dir}/gsim/gsim_catch_id_list_up.txt',gsim_id_list_up,fmt='%s')\n",
    "np.savetxt(f'{out_dir}/gsim/gsim_catch_id_list_lo.txt',gsim_id_list_lo,fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5ad5a6-4a80-4876-b815-e22d0f4a1442",
   "metadata": {},
   "source": [
    "### 2.2 Preprocess data \n",
    "\n",
    "The GSIM yearly discharge timeseries are stored in *.year* files. A detailed explanation of the column names is provided in Table 3 and 4 in https://essd.copernicus.org/articles/10/787/2018/. Here we preprocess these data into readable *.csv* files for each catchment. The preprocessing function *preprocess_gsim_discharge* is defined in the file *f_preprocess_discharge.py*. With this function we generate for each catchment a file with the yearly discharge timeseries and a file with the specifications of the catchment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "23ce5bd9-8f79-4a45-a598-04ddcb1d18fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make output directories\n",
    "if not os.path.exists(f'{out_dir}/gsim/timeseries'):\n",
    "    os.makedirs(f'{out_dir}/gsim/timeseries')\n",
    "\n",
    "if not os.path.exists(f'{out_dir}/gsim/timeseries_selected'):\n",
    "    os.makedirs(f'{out_dir}/gsim/timeseries_selected')\n",
    "    \n",
    "if not os.path.exists(f'{out_dir}/gsim/characteristics'):\n",
    "    os.makedirs(f'{out_dir}/gsim/characteristics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8b1495ea-6308-4d4d-ae77-8722d9683281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS WITH SLURM\n",
    "\n",
    "gsim_id_list_lo = np.loadtxt(f'{out_dir}/gsim/gsim_catch_id_list_lo.txt',dtype=str) \n",
    "\n",
    "# define folder with discharge timeseries data\n",
    "fol_in = f'{data_dir}/GSIM_data/GSIM_indices/TIMESERIES/yearly/'\n",
    "\n",
    "# define output folder\n",
    "fol_out = f'{out_dir}/gsim/'\n",
    "\n",
    "catch_list = gsim_id_list_lo\n",
    "fol_in_list = [fol_in] * len(catch_list)\n",
    "fol_out_list = [fol_out] * len(catch_list)\n",
    "\n",
    "run_function_parallel(catch_list,fol_in_list,fol_out_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bb6f506c-3571-497d-91fb-029c7b9a1d7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>long.org</th>\n",
       "      <th>lat.org</th>\n",
       "      <th>long.new</th>\n",
       "      <th>lat.new</th>\n",
       "      <th>dist.km</th>\n",
       "      <th>area.meta</th>\n",
       "      <th>area.est</th>\n",
       "      <th>quality</th>\n",
       "      <th>altitude.meta</th>\n",
       "      <th>altitude.dem</th>\n",
       "      <th>...</th>\n",
       "      <th>slt.q2</th>\n",
       "      <th>slt.q3</th>\n",
       "      <th>slt.max</th>\n",
       "      <th>soil.type</th>\n",
       "      <th>tp.mean</th>\n",
       "      <th>tp.min</th>\n",
       "      <th>tp.q1</th>\n",
       "      <th>tp.q2</th>\n",
       "      <th>tp.q3</th>\n",
       "      <th>tp.max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gsim.no</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AF_0000001</th>\n",
       "      <td>68.3000</td>\n",
       "      <td>36.9833</td>\n",
       "      <td>68.2979</td>\n",
       "      <td>36.9854</td>\n",
       "      <td>0.2987</td>\n",
       "      <td>37100.0</td>\n",
       "      <td>36956.9777</td>\n",
       "      <td>High</td>\n",
       "      <td>320.0</td>\n",
       "      <td>331.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>No dominant class</td>\n",
       "      <td>1.53592</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.558417</td>\n",
       "      <td>19.7089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AF_0000002</th>\n",
       "      <td>68.8333</td>\n",
       "      <td>36.7000</td>\n",
       "      <td>68.8271</td>\n",
       "      <td>36.7021</td>\n",
       "      <td>0.5997</td>\n",
       "      <td>24820.0</td>\n",
       "      <td>23392.2953</td>\n",
       "      <td>Medium</td>\n",
       "      <td>401.0</td>\n",
       "      <td>379.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>No dominant class</td>\n",
       "      <td>1.36854</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.369006</td>\n",
       "      <td>19.2480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AF_0000003</th>\n",
       "      <td>68.6667</td>\n",
       "      <td>36.1000</td>\n",
       "      <td>68.6646</td>\n",
       "      <td>36.1062</td>\n",
       "      <td>0.7143</td>\n",
       "      <td>19740.0</td>\n",
       "      <td>19476.9207</td>\n",
       "      <td>High</td>\n",
       "      <td>562.0</td>\n",
       "      <td>583.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>No dominant class</td>\n",
       "      <td>1.29468</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.253164</td>\n",
       "      <td>19.0594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AF_0000004</th>\n",
       "      <td>68.6000</td>\n",
       "      <td>35.6000</td>\n",
       "      <td>68.6062</td>\n",
       "      <td>35.5979</td>\n",
       "      <td>0.6069</td>\n",
       "      <td>12610.0</td>\n",
       "      <td>12457.6447</td>\n",
       "      <td>High</td>\n",
       "      <td>899.0</td>\n",
       "      <td>963.0</td>\n",
       "      <td>...</td>\n",
       "      <td>28.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>Regosols</td>\n",
       "      <td>1.86169</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.316522</td>\n",
       "      <td>2.954909</td>\n",
       "      <td>18.5653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AF_0000005</th>\n",
       "      <td>67.9167</td>\n",
       "      <td>35.3000</td>\n",
       "      <td>67.9146</td>\n",
       "      <td>35.3021</td>\n",
       "      <td>0.3012</td>\n",
       "      <td>3795.0</td>\n",
       "      <td>3684.0183</td>\n",
       "      <td>High</td>\n",
       "      <td>1588.0</td>\n",
       "      <td>1579.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>Regosols</td>\n",
       "      <td>1.77934</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.979516</td>\n",
       "      <td>16.6662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZW_0000079</th>\n",
       "      <td>29.3667</td>\n",
       "      <td>-20.7833</td>\n",
       "      <td>29.3687</td>\n",
       "      <td>-20.7813</td>\n",
       "      <td>0.3043</td>\n",
       "      <td>2963.0</td>\n",
       "      <td>2968.8640</td>\n",
       "      <td>High</td>\n",
       "      <td>NaN</td>\n",
       "      <td>969.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>Luvisols</td>\n",
       "      <td>2.46013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.009445</td>\n",
       "      <td>16.8299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZW_0000080</th>\n",
       "      <td>29.4167</td>\n",
       "      <td>-20.6000</td>\n",
       "      <td>29.4229</td>\n",
       "      <td>-20.5938</td>\n",
       "      <td>0.9437</td>\n",
       "      <td>453.0</td>\n",
       "      <td>447.1602</td>\n",
       "      <td>High</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1059.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>Luvisols</td>\n",
       "      <td>3.31634</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.162149</td>\n",
       "      <td>5.414657</td>\n",
       "      <td>15.1111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZW_0000081</th>\n",
       "      <td>29.9333</td>\n",
       "      <td>-20.8333</td>\n",
       "      <td>29.9312</td>\n",
       "      <td>-20.8354</td>\n",
       "      <td>0.3194</td>\n",
       "      <td>1929.0</td>\n",
       "      <td>1974.2730</td>\n",
       "      <td>High</td>\n",
       "      <td>NaN</td>\n",
       "      <td>856.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>Luvisols</td>\n",
       "      <td>3.10230</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.243050</td>\n",
       "      <td>5.294613</td>\n",
       "      <td>16.2139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZW_0000082</th>\n",
       "      <td>29.6000</td>\n",
       "      <td>-20.6000</td>\n",
       "      <td>29.5979</td>\n",
       "      <td>-20.5979</td>\n",
       "      <td>0.3196</td>\n",
       "      <td>277.0</td>\n",
       "      <td>259.3699</td>\n",
       "      <td>Medium</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1038.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>Luvisols</td>\n",
       "      <td>2.68539</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.190788</td>\n",
       "      <td>14.6615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZW_0000083</th>\n",
       "      <td>29.9833</td>\n",
       "      <td>-22.2167</td>\n",
       "      <td>29.9812</td>\n",
       "      <td>-22.2229</td>\n",
       "      <td>0.7221</td>\n",
       "      <td>208784.0</td>\n",
       "      <td>204010.9000</td>\n",
       "      <td>High</td>\n",
       "      <td>NaN</td>\n",
       "      <td>441.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>Luvisols</td>\n",
       "      <td>3.52895</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.965397</td>\n",
       "      <td>6.191565</td>\n",
       "      <td>21.3273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30959 rows × 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            long.org  lat.org  long.new  lat.new  dist.km  area.meta  \\\n",
       "gsim.no                                                                \n",
       "AF_0000001   68.3000  36.9833   68.2979  36.9854   0.2987    37100.0   \n",
       "AF_0000002   68.8333  36.7000   68.8271  36.7021   0.5997    24820.0   \n",
       "AF_0000003   68.6667  36.1000   68.6646  36.1062   0.7143    19740.0   \n",
       "AF_0000004   68.6000  35.6000   68.6062  35.5979   0.6069    12610.0   \n",
       "AF_0000005   67.9167  35.3000   67.9146  35.3021   0.3012     3795.0   \n",
       "...              ...      ...       ...      ...      ...        ...   \n",
       "ZW_0000079   29.3667 -20.7833   29.3687 -20.7813   0.3043     2963.0   \n",
       "ZW_0000080   29.4167 -20.6000   29.4229 -20.5938   0.9437      453.0   \n",
       "ZW_0000081   29.9333 -20.8333   29.9312 -20.8354   0.3194     1929.0   \n",
       "ZW_0000082   29.6000 -20.6000   29.5979 -20.5979   0.3196      277.0   \n",
       "ZW_0000083   29.9833 -22.2167   29.9812 -22.2229   0.7221   208784.0   \n",
       "\n",
       "               area.est quality  altitude.meta  altitude.dem  ... slt.q2  \\\n",
       "gsim.no                                                       ...          \n",
       "AF_0000001   36956.9777    High          320.0         331.0  ...    0.0   \n",
       "AF_0000002   23392.2953  Medium          401.0         379.0  ...    0.0   \n",
       "AF_0000003   19476.9207    High          562.0         583.0  ...    0.0   \n",
       "AF_0000004   12457.6447    High          899.0         963.0  ...   28.0   \n",
       "AF_0000005    3684.0183    High         1588.0        1579.0  ...    0.0   \n",
       "...                 ...     ...            ...           ...  ...    ...   \n",
       "ZW_0000079    2968.8640    High            NaN         969.0  ...    0.0   \n",
       "ZW_0000080     447.1602    High            NaN        1059.0  ...   10.0   \n",
       "ZW_0000081    1974.2730    High            NaN         856.0  ...    9.0   \n",
       "ZW_0000082     259.3699  Medium            NaN        1038.0  ...    0.0   \n",
       "ZW_0000083  204010.9000    High            NaN         441.0  ...    8.0   \n",
       "\n",
       "            slt.q3  slt.max          soil.type  tp.mean  tp.min  tp.q1  \\\n",
       "gsim.no                                                                  \n",
       "AF_0000001    36.0     49.0  No dominant class  1.53592    -1.0    0.0   \n",
       "AF_0000002    35.0     48.0  No dominant class  1.36854    -1.0    0.0   \n",
       "AF_0000003    34.0     48.0  No dominant class  1.29468    -1.0    0.0   \n",
       "AF_0000004    35.0     45.0           Regosols  1.86169    -1.0    0.0   \n",
       "AF_0000005    34.0     44.0           Regosols  1.77934     0.0    0.0   \n",
       "...            ...      ...                ...      ...     ...    ...   \n",
       "ZW_0000079    12.0     21.0           Luvisols  2.46013     0.0    0.0   \n",
       "ZW_0000080    11.0     21.0           Luvisols  3.31634     0.0    0.0   \n",
       "ZW_0000081    11.0     20.0           Luvisols  3.10230     0.0    0.0   \n",
       "ZW_0000082    10.0     20.0           Luvisols  2.68539     0.0    0.0   \n",
       "ZW_0000083    11.0     30.0           Luvisols  3.52895     0.0    0.0   \n",
       "\n",
       "               tp.q2     tp.q3   tp.max  \n",
       "gsim.no                                  \n",
       "AF_0000001  0.000000  2.558417  19.7089  \n",
       "AF_0000002  0.000000  2.369006  19.2480  \n",
       "AF_0000003  0.000000  2.253164  19.0594  \n",
       "AF_0000004  1.316522  2.954909  18.5653  \n",
       "AF_0000005  0.000000  2.979516  16.6662  \n",
       "...              ...       ...      ...  \n",
       "ZW_0000079  0.000000  5.009445  16.8299  \n",
       "ZW_0000080  4.162149  5.414657  15.1111  \n",
       "ZW_0000081  3.243050  5.294613  16.2139  \n",
       "ZW_0000082  0.000000  5.190788  14.6615  \n",
       "ZW_0000083  3.965397  6.191565  21.3273  \n",
       "\n",
       "[30959 rows x 83 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RUN THIS ALSO WITH SLURM\n",
    "\n",
    "\n",
    "#%%# select catchments\n",
    "# - area quality high or medium\n",
    "# - timeseries after 1980\n",
    "# - amount of days <250 -> nan\n",
    "# - remove nans\n",
    "# - >10 years data\n",
    "\n",
    "# make list with catchment ids that are ok:\n",
    "df = pd.read_csv(f'{data_dir}/GSIM_data/GSIM_metadata/GSIM_catalog/GSIM_catchment_characteristics.csv',index_col=0) \n",
    "for catch_id in gsim_id_list_lo[0:5]:\n",
    "    select_catchments(out_dir,catch_id,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5bf96625-52f8-4e4f-aec1-c6924a44552c",
   "metadata": {},
   "outputs": [],
   "source": [
    "catch_id = gsim_id_list_lo[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7ed32f87-f2e4-449d-bfd4-8cee2777844a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_catchments(out_dir,catch_id,df):\n",
    "    a = pd.read_csv(f'{out_dir}/gsim/characteristics/{catch_id}.csv',index_col=0)\n",
    "    a['start_year'] = pd.to_datetime(a['start_year'])\n",
    "    a['end_year'] = pd.to_datetime(a['end_year'])\n",
    "    k = a['gsim.no'][0]   \n",
    "\n",
    "    # if end year earlier than 1980 go to next iteration\n",
    "    t_1980 = a['end_year'][0]< datetime(year=1980,month=1,day=1)\n",
    "\n",
    "    # check timeseries\n",
    "    b = pd.read_csv(f'{out_dir}/gsim/timeseries/{catch_id}.csv',index_col=0)\n",
    "\n",
    "    # later than 1980\n",
    "    b = b.loc['1980-12-31':]\n",
    "\n",
    "    # set to nan if nr available days <250\n",
    "    b.Q[b['nr_av_days']<250] = np.nan\n",
    "\n",
    "    # drop nan years\n",
    "    b = b.dropna(axis=0)\n",
    "\n",
    "    # check length of timeseries\n",
    "    len_b = len(b)\n",
    "\n",
    "    # >10 years data\n",
    "    t_length = len_b>10\n",
    "\n",
    "    # area quality\n",
    "    a_qual = (df.loc[k]['quality']=='High') or (df.loc[k]['quality']=='Medium')\n",
    "\n",
    "    if (t_length) & (a_qual) & (t_1980):\n",
    "        b.to_csv(f'{out_dir}/gsim/timeseries_selected/{catch_id}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fcf93d30-3dba-4334-9724-8b0b27f84854",
   "metadata": {},
   "outputs": [],
   "source": [
    "for catch_id in gsim_id_list_lo[0:5]:\n",
    "    select_catchments(catch_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29d5cdf-ac32-4225-bb40-b2198198e1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make list of catch ids in timeseries_selected folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0535e2ff-724f-45a8-ab05-31922433b1fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catch_id_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6e8c0f-a4f8-4bea-85f6-1cf2113ba51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.asarray(catch_id)\n",
    "s = s.astype(str)\n",
    "np.savetxt(f'{folder}GSIM_processed_data/catch_id_selected_mmd.txt',s,fmt = '%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7c08be-0f9f-40c6-8edc-a3982826ef65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% read catch_id_selected\n",
    "a = np.loadtxt(f'{folder}GSIM_processed_data/catch_id_selected_mmd.txt',dtype=str)   \n",
    "\n",
    "# convert upper case letters to lower case because shapefiles use lower case\n",
    "z = np.zeros(len(a),dtype='<U10')\n",
    "for i in range(len(z)):\n",
    "    k = a[i].astype(str)\n",
    "    k = k.lower()\n",
    "    z[i] = k\n",
    "\n",
    "np.savetxt(f'{folder}GSIM_processed_data/catch_id_selected_mmd_lowercase.txt',z,fmt = '%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73ef949-4270-4a15-aaa3-d2155aa0ad34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4ebda8-020a-4eca-8c7c-b19698b31c2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "140fd899-95d5-4490-b13c-aa448ba8e410",
   "metadata": {},
   "source": [
    "### 3. Make lists of catchment IDs\n",
    "The module calculates catchment root zone storage capacities for a large sample of catchments. Here we save the catchment names in a .txt file, for later use in the scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47d31e59-49f8-4e94-af71-60abddffb7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the filenames of catchment shapefiles\n",
    "shape_dir = Path(f'{data_dir}/shapes/')\n",
    "shapefiles = glob.glob(f\"{shape_dir}/*shp\")\n",
    "\n",
    "# make an empty list\n",
    "catch_id_list = []\n",
    "\n",
    "# loop over the catchment shapefiles, extract the catchment id and store in the empty list\n",
    "for i in shapefiles:\n",
    "    catch_id_list.append(Path(i).name.split('.')[0])\n",
    "    \n",
    "# save the catchment id list in your output directory\n",
    "np.savetxt(f'{out_dir}/catch_id_list.txt',catch_id_list,fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3c8f9cd-45af-4bc8-bdf8-88374d7c30db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['br_0000495', 'fr_0000326', 'us_0002247', 'br_0000208', 'ca_0002435', 'zw_0000005']\n"
     ]
    }
   ],
   "source": [
    "# print catch_id_list as check\n",
    "print(catch_id_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baf7036-1515-444d-83f0-b7d2b6bead68",
   "metadata": {},
   "source": [
    "### 4. GSIM discharge data\n",
    "### 4.1 Preprocess data \n",
    "\n",
    "The GSIM yearly discharge timeseries are stored in *.year* files. A detailed explanation of the column names is provided in Table 3 and 4 in https://essd.copernicus.org/articles/10/787/2018/. Here we preprocess these data into readable *.csv* files for each catchment. The preprocessing function *preprocess_gsim_discharge* is defined in the file *f_preprocess_discharge.py*. With this function we generate for each catchment a file with the yearly discharge timeseries and a file with the specifications of the catchment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a50094bb-f752-4e43-8dd9-b716cb5f0dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make output directories\n",
    "if not os.path.exists(f'{out_dir}/discharge/timeseries'):\n",
    "    os.makedirs(f'{out_dir}/discharge/timeseries')\n",
    "    \n",
    "if not os.path.exists(f'{out_dir}/discharge/characteristics'):\n",
    "    os.makedirs(f'{out_dir}/discharge/characteristics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6c3a68a-ac3b-4897-88e6-d39721d5335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define folder with discharge timeseries data\n",
    "fol_in = f'{data_dir}/gsim_discharge/'\n",
    "\n",
    "# define output folder\n",
    "fol_out = f'{out_dir}/discharge/'\n",
    "\n",
    "# run preprocess_gsim_discharge function (defined in f_preprocess_discharge.py) for all catchments in catch_id_list\n",
    "for catch_id in catch_id_list:\n",
    "    preprocess_gsim_discharge(catch_id, fol_in, fol_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c05b51e-4007-4864-8c3d-f443e9bf1365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Q</th>\n",
       "      <th>sd_mmd</th>\n",
       "      <th>iqr_mmd</th>\n",
       "      <th>min_mmd</th>\n",
       "      <th>max_mmd</th>\n",
       "      <th>min7_mmd</th>\n",
       "      <th>max7_mmd</th>\n",
       "      <th>doymin</th>\n",
       "      <th>doymax</th>\n",
       "      <th>doy7min</th>\n",
       "      <th>doy7max</th>\n",
       "      <th>nr_mis_days</th>\n",
       "      <th>nr_av_days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1964-12-31</td>\n",
       "      <td>2.782319</td>\n",
       "      <td>0.226279</td>\n",
       "      <td>0.264220</td>\n",
       "      <td>2.510092</td>\n",
       "      <td>3.170642</td>\n",
       "      <td>2.547837</td>\n",
       "      <td>3.114024</td>\n",
       "      <td>356.0</td>\n",
       "      <td>334.0</td>\n",
       "      <td>361.0</td>\n",
       "      <td>340.0</td>\n",
       "      <td>333.0</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1965-12-31</td>\n",
       "      <td>3.429440</td>\n",
       "      <td>1.101704</td>\n",
       "      <td>1.770275</td>\n",
       "      <td>1.587963</td>\n",
       "      <td>6.816881</td>\n",
       "      <td>1.587963</td>\n",
       "      <td>5.910983</td>\n",
       "      <td>356.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>362.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>365.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1966-12-31</td>\n",
       "      <td>2.563471</td>\n",
       "      <td>0.907277</td>\n",
       "      <td>1.030459</td>\n",
       "      <td>1.500771</td>\n",
       "      <td>14.400000</td>\n",
       "      <td>1.587963</td>\n",
       "      <td>4.642726</td>\n",
       "      <td>17.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>365.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1967-12-31</td>\n",
       "      <td>2.727136</td>\n",
       "      <td>0.985530</td>\n",
       "      <td>1.638165</td>\n",
       "      <td>1.004037</td>\n",
       "      <td>4.967339</td>\n",
       "      <td>1.004037</td>\n",
       "      <td>4.529489</td>\n",
       "      <td>281.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>365.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1968-12-31</td>\n",
       "      <td>2.812898</td>\n",
       "      <td>0.556125</td>\n",
       "      <td>0.930055</td>\n",
       "      <td>1.664587</td>\n",
       "      <td>3.989725</td>\n",
       "      <td>1.721206</td>\n",
       "      <td>3.566972</td>\n",
       "      <td>22.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>366.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date         Q    sd_mmd   iqr_mmd   min_mmd    max_mmd  min7_mmd  \\\n",
       "0  1964-12-31  2.782319  0.226279  0.264220  2.510092   3.170642  2.547837   \n",
       "1  1965-12-31  3.429440  1.101704  1.770275  1.587963   6.816881  1.587963   \n",
       "2  1966-12-31  2.563471  0.907277  1.030459  1.500771  14.400000  1.587963   \n",
       "3  1967-12-31  2.727136  0.985530  1.638165  1.004037   4.967339  1.004037   \n",
       "4  1968-12-31  2.812898  0.556125  0.930055  1.664587   3.989725  1.721206   \n",
       "\n",
       "   max7_mmd  doymin  doymax  doy7min  doy7max  nr_mis_days  nr_av_days  \n",
       "0  3.114024   356.0   334.0    361.0    340.0        333.0        33.0  \n",
       "1  5.910983   356.0   121.0    362.0    126.0          0.0       365.0  \n",
       "2  4.642726    17.0   159.0      1.0    165.0          0.0       365.0  \n",
       "3  4.529489   281.0   151.0    300.0    151.0          0.0       365.0  \n",
       "4  3.566972    22.0   175.0     26.0    176.0          0.0       366.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print output discharge dataframe for catchment [0]\n",
    "catch_id = catch_id_list[0]\n",
    "c = pd.read_csv(f'{fol_out}/timeseries/{catch_id}.csv')\n",
    "c.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f821fae5-8121-4375-bc8e-d5e03700abcb",
   "metadata": {},
   "source": [
    "### 4.2 Select catchments\n",
    "Here we select catchments based on the available data, following these criteria:\n",
    "- Timeseries should be longer than 10 years after 1980\n",
    "- Area quality given in GSIM database medium or high\n",
    "- Mean Q is not NaN\n",
    "- \n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ba0b3ba-a18f-434d-bd6b-7b9697849937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BR_0000495' 'FR_0000326' 'US_0002247' 'BR_0000208' 'CA_0002435'\n",
      " 'ZW_0000005']\n"
     ]
    }
   ],
   "source": [
    "s = select_catchments_q(out_dir)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2859eda9-f3b3-4c5c-a6cc-c7cdbb62732c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select catchments part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ffa5c8-eabd-4439-9018-bd5185d61a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% for selected catchments have a check on:\n",
    "    # - if amount of daily values in a year < 200 -> year=nan\n",
    "    # - amount of nan years in timeseries\n",
    "    # - if too many nan years -> exclude\n",
    "    \n",
    "# selected catchment names\n",
    "a = np.loadtxt(f'{out_dir}/catch_id_list_selected_q.txt',dtype=str) \n",
    "s = [] #list with new selected catchment names\n",
    "\n",
    "# loop over pre-selected catchments\n",
    "for i in range(len(a)):\n",
    "    b = pd.read_csv(f'{folder}GSIM_processed_data/catchment_yearly/mm_day/'+str(a[i])+'.csv',index_col=0)\n",
    "    b.index = pd.to_datetime(b.index)\n",
    "    \n",
    "    # select timeseries from 1980 onwards\n",
    "    b_end = b.index[-1]\n",
    "    if (b.index[0]<datetime.datetime(year=1980,month=1,day=1)):\n",
    "        b = b.loc['1980-12-31':str(b_end)]\n",
    "    \n",
    "    # change to nan if nr_av_days < 200\n",
    "    b.mean_mmd[b.nr_av_days < 200] = np.nan\n",
    "\n",
    "    # count amount of nan years\n",
    "    nan_sum = b.mean_mmd.isna().sum()\n",
    "    if nan_sum < 0.2*len(b):\n",
    "        s.append(a[i])\n",
    "    print(i)\n",
    "    \n",
    "    b.to_csv(f'{folder}GSIM_processed_data/catchment_selected_yearly/'+str(a[i])+'.csv')\n",
    "\n",
    "s = np.asarray(s)\n",
    "s = s.astype(str)\n",
    "np.savetxt(f'{folder}GSIM_processed_data/catch_id_selected2_mmd.txt',s,fmt = '%s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fe796c-118c-4e6c-8511-59be1b6a93c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7b0ae1-c42e-40dd-9b56-2bb56a4a670e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% read catch_id_selected\n",
    "a = np.loadtxt(f'{folder}GSIM_processed_data/catch_id_selected2_mmd.txt',dtype=str)   \n",
    "\n",
    "# convert upper case letters to lower case because shapefiles use lower case\n",
    "z = np.zeros(len(a),dtype='<U10')\n",
    "for i in range(len(z)):\n",
    "    k = a[i].astype(str)\n",
    "    k = k.lower()\n",
    "    z[i] = k\n",
    "\n",
    "np.savetxt(f'{folder}GSIM_processed_data/catch_id_selected2_mmd_lowercase.txt',z,fmt = '%s')\n",
    "\n",
    "\n",
    "#%% map the catchments with useful timeseries\n",
    "\n",
    "# append all shapefilenames of selected catchments\n",
    "shapefiles = []\n",
    "for i in range(len(z)):\n",
    "    shapefiles.append(str(folder)+'GSIM_data/GSIM_metadata/GSIM_catchments/'+str(z[i])+'.shp')\n",
    "\n",
    "\n",
    "shapefiles2=[]\n",
    "for i in range(len(shapefiles)):\n",
    "    myfile = Path(str(shapefiles[i]))\n",
    "    if myfile.is_file():\n",
    "        shapefiles2.append(myfile)\n",
    "        \n",
    "\n",
    "# concat shapefiles into one geodataframe  \n",
    "gdf = pd.concat([\n",
    "    gpd.read_file(shp)\n",
    "    for shp in shapefiles2\n",
    "]).pipe(gpd.GeoDataFrame)\n",
    "\n",
    "gdf.to_file(str(folder)+'GSIM_processed_data/merged_gpd_selected_catchments.shp')\n",
    "\n",
    "    \n",
    "#%% map gdf\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "gdf = gpd.read_file(str(folder)+'GSIM_processed_data/merged_gpd_selected_catchments.shp')\n",
    "\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "ax = fig.add_subplot(111)\n",
    "world.plot(ax=ax, edgecolor='black',linestyle=':', facecolor='white')\n",
    "\n",
    "gdf.plot(edgecolor='black',\n",
    "        linewidth=0.2,\n",
    "        ax=ax)\n",
    "ax.set_title('#catchments: '+str(len(gdf)),size=30)\n",
    "fig.savefig(str(folder)+'/GSIM_processed_data/figures/mapping_catchments.jpg',bbox_inches='tight')\n",
    "\n",
    "    \n",
    "    \n",
    "#%% add australia shapes to gdf\n",
    "gdf = gpd.read_file(str(folder)+'GSIM_processed_data/merged_gpd_selected_catchments.shp')\n",
    "aus = gpd.read_file('P:/Fransje/LSM root zone/CAMELS_AUS/02_location_boundary_area/shp/CAMELS_AUS_Boundaries_adopted.shp')\n",
    "gdf_aus = pd.concat([gdf,aus])    \n",
    "    \n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "ax = fig.add_subplot(111)\n",
    "world.plot(ax=ax, edgecolor='black',linestyle=':', facecolor='white')\n",
    "\n",
    "gdf_aus.plot(edgecolor='black',\n",
    "        linewidth=0.2,\n",
    "        ax=ax)\n",
    "ax.set_title('#catchments: '+str(len(gdf_aus)),size=30)\n",
    "fig.savefig(str(folder)+'/GSIM_processed_data/figures/mapping_catchments_aus.jpg',bbox_inches='tight')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4138fe53-5f10-4508-9e16-4b1c41b22cd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe425556-a09e-44d5-b874-b19bd91a73b7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0516da6d-192d-4f7d-b3c5-0ffe8db2f7a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1bfb49e0-e014-45eb-ac49-200b78d4c9b5",
   "metadata": {},
   "source": [
    "### 5. From gridded data to catchment timeseries\n",
    "For this step go to the notebook *run_script_grid_to_catchments*. This part is run in another notebook. The output data of this script can be found in *work_dir/output/forcing_timeseries*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7db8dd-39f3-4b02-93fc-2faaf74cb75b",
   "metadata": {},
   "source": [
    "### 6. Google earth engine for catchment characteristics\n",
    "For this step go to the notebook *run_script_earthengine*. This part is run in another notebook. The output data of this script can be found in *work_dir/output/earth_engine_timeseries*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecbdbeb-4659-482d-802d-dccd600b1658",
   "metadata": {},
   "source": [
    "### 7. Catchment descriptor variables\n",
    "For the global root zone storage capacity estimation, we need to calculate catchment descriptor variables. These descriptors can be climatological variables (e.g. mean precipitation (p_mean); seasonality of precipitation (si_p); timelag between maximum P and Ep (phi)) or landscape variables (e.g. mean treecover (tc); mean elevation (h_mean)). A detailed list of all the descriptors considered is provided here xxxxx.\\\n",
    "To calculate the catchment descriptor variables we use the *catch_characteristics* function from the *f_catch_characteristics.py* file. In this function you specify the variables of interest, the catchment ID and your in- and output folders. Then, based on all the timeseries you have generated in the preceding codes it will return a table with the catchment descriptor variables for all your catchments (that is saved as csv in your *work_dir/catchment_characteristics.csv*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ab7412-ed47-47a1-b5be-cff509d48929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define in and output folder\n",
    "fol_in=f'{work_dir}/output/'\n",
    "fol_out=f'{work_dir}/output/'\n",
    "\n",
    "# define variables of interest\n",
    "var=['p_mean','ep_mean','q_mean','t_mean','ai','si_p','si_ep','phi','tc','ntc','nonveg']\n",
    "\n",
    "# run catch_characteristics (defined in f_catch_characteristics.py) for the catchments in your catch_id_list\n",
    "catch_characteristics(var, catch_id_list, fol_in, fol_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95319679-461d-4b75-aca0-bbf9cfbf692c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine catchment geometries in single shapefile\n",
    "shape_dir = f'{data_dir}/shapes/'\n",
    "out_dir = f'{work_dir}/output'\n",
    "geo_catchments(shape_dir,out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f2a634-2971-4984-9de2-993faad4ac48",
   "metadata": {},
   "source": [
    "## check also for WB and for AREA (<10000km2) - see catchment_waterbalance.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f9e214-04ed-499e-9e45-845bc7d74c23",
   "metadata": {},
   "source": [
    "### 8. Calculate root zone storage capacity\n",
    "Here we calculate the catchment root zone storage capacity (Sr) based on catchment water balances. First, catchment root zone storage deficits (Sd) are computed as the cumulative difference between P and Et (transpiration). The result of one catchment is visualised in a figure. Second, the Sr is then calculated based on an extreme value analysis of the storage deficits for different return periods. A detailed description of this method can be found here xxxxxx.\n",
    "\n",
    "Here we use the *run_sd_calculation* and *run_sr_calculation* functions from the *f_sr_calculation* file. The Sd result of one catchment is visualised using *plot_sd*. The Sr results are merged using the *merge_sr* function and visualised using the *plot_sr* function. The output of both storage deficit and Sr calculations are saved in your *work_dir/output/sr_calculation*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409d935d-0bc8-473f-9018-a12a7af16576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make output directories\n",
    "if not os.path.exists(f'{work_dir}/output/sr_calculation/sd_catchments'):\n",
    "    os.makedirs(f'{work_dir}/output/sr_calculation/sd_catchments')\n",
    "    \n",
    "if not os.path.exists(f'{work_dir}/output/sr_calculation/sr_catchments'):\n",
    "    os.makedirs(f'{work_dir}/output/sr_calculation/sr_catchments')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4bed7a-fd38-422a-9577-e20cf6be7bac",
   "metadata": {},
   "source": [
    "Calculate storage deficits using the *run_sd_calculation* function from *f_sr_calculation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0641804-4e6a-4a1c-ad7d-fceaec17bc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define directories\n",
    "pep_dir = f'{work_dir}/output/forcing_timeseries/processed/daily'\n",
    "q_dir = f'{work_dir}/output/discharge/timeseries'\n",
    "out_dir = f'{work_dir}/output/sr_calculation/sd_catchments'\n",
    "\n",
    "# run sd calculation for all catchments in catch_id_list\n",
    "for catch_id in catch_id_list:\n",
    "    run_sd_calculation(catch_id, pep_dir, q_dir, out_dir)\n",
    "    \n",
    "#comRuud: print to screen what is being created (for some reason I did not get an Sd for the US catchment, but no error!)\n",
    "# Fransje: this is correct, incorrect water balance in us catchment -> no sd calculated. add flag or so in table??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84c3b66-6d7e-4081-bfb1-6cec197d70db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot sd example - use the first catchment from catch_id_list\n",
    "sd_dir = f'{work_dir}/output/sr_calculation/sd_catchments'\n",
    "plot_sd(catch_id_list[0], sd_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893710c3-63f6-482d-b348-9a434e79e787",
   "metadata": {},
   "source": [
    "Calculate Sr using the *run_sr_calculation* function from *f_sr_calculation* and merge the catchment Sr values into one dataframe with *merge_sr_catchments*. The functions return a table with the catchment Sr values for the different return periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882c1d56-675c-49a4-89e0-b1c5b40054c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define directories\n",
    "sd_dir = f'{work_dir}/output/sr_calculation/sd_catchments'\n",
    "out_dir = f'{work_dir}/output/sr_calculation/sr_catchments'\n",
    "\n",
    "# define return periods\n",
    "rp_array = [2,3,5,10,20,30,40,50,60]\n",
    "\n",
    "# run sr calculation for all catchments in catch_id_list\n",
    "for catch_id in catch_id_list:\n",
    "    run_sr_calculation(catch_id, rp_array, sd_dir, out_dir)\n",
    "    \n",
    "# merge catchment sr dataframes into one dataframe\n",
    "sr_dir = f'{work_dir}/output/sr_calculation/sr_catchments'\n",
    "out_dir = f'{work_dir}/output/sr_calculation/'\n",
    "merge_sr_catchments(sr_dir,out_dir)\n",
    "\n",
    "#comRuud: multiple Sr have been created (for different return periods? But values are the same, is that correct?) print to screen and tell what it should look like"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce395fd-fe07-444f-942c-6d2cae88823e",
   "metadata": {},
   "source": [
    "Mapping Sr using the *plot_sr* function from *f_sr_calculation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c841ffa-b51a-4e41-b34b-015b0c14572e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_file = f'{work_dir}/output/sr_calculation/sr_all_catchments.csv'\n",
    "shp_file = f'{work_dir}/output/geo_catchments.shp'\n",
    "rp=20\n",
    "plot_sr(shp_file,sr_file,rp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765ed214-c93f-40bd-bf5e-7e215dfc8066",
   "metadata": {},
   "source": [
    "### 9. Regression\n",
    "\n",
    "**move this to different script? to separate 'preprocessing' and 'analysis'?**\n",
    "\n",
    "Here we run the linear regression model to predict the catchment Sr values based on the descriptor parameters. We use the *f_regression* function to calculate the linear regression parameters for the considered catchments.\n",
    "We use the treecover data to separate the regression for high and low vegetation, the threshold values for tree cover (tc), non tree cover (ntc) and no-vegetation (nonveg) define this separation.\n",
    "\n",
    "The output is a figure showing the estimated (step 8) and predicted (from regression) Sr values and a table with the regression parameter values, some statistics for the regression performance and the threshold values for tree cover. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549b3ebf-5db9-4eda-9832-2439c8d960c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the catchment characteristics and sr tables\n",
    "cc_df = pd.read_csv(f'{work_dir}/output/catchment_characteristics.csv',index_col=0)\n",
    "sr_df = pd.read_csv(f'{work_dir}/output/sr_calculation/sr_all_catchments.csv',index_col=0)\n",
    "\n",
    "# define the descriptor variables\n",
    "dpar = ['p_mean','ep_mean','t_mean','si_p']\n",
    "\n",
    "# return period of Sr estimate\n",
    "rp = 20\n",
    "\n",
    "# define the vegetation thresholds for the regression\n",
    "tc_th, ntc_th, nonveg_th = 10, 0, 0\n",
    "\n",
    "# run the regression (r_regression in f_regression.py)\n",
    "run_regression(cc_df, sr_df, dpar, rp, tc_th, ntc_th, nonveg_th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3784f57-4033-469b-9191-7296fd3d89dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
